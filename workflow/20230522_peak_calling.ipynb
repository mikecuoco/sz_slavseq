{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak calling testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, DEVNULL\n",
    "from tempfile import TemporaryDirectory, NamedTemporaryFile\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from io import StringIO\n",
    "from collections import deque, namedtuple, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyranges as pr\n",
    "import pysam\n",
    "from scripts.get_labels import read_knrgl\n",
    "from myutils.rmsk import read_rmsk\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in repeatmasker output\n",
    "rmsk = read_rmsk(\"/iblm/netapp/data4/mcuoco/sz_slavseq/resources/hs38d1.fa.out\")\n",
    "\n",
    "rep_names = [\n",
    "    \"L1HS_3end\",\n",
    "    \"L1PA2_3end\",\n",
    "    \"L1PA3_3end\",\n",
    "    \"L1PA4_3end\",\n",
    "    \"L1PA5_3end\",\n",
    "    \"L1PA6_3end\",\n",
    "]\n",
    "\n",
    "rmsk = rmsk.loc[(rmsk[\"repName\"].isin(rep_names)) & (rmsk[\"repEnd\"] > 860), :]\n",
    "rmsk = rmsk.apply(\n",
    "    lambda x: x\n",
    "    if (x[\"strand\"] == \"+\" and x[\"repStart\"] < 765)\n",
    "    or (x[\"strand\"] == \"-\" and x[\"repLeft\"] < 765)\n",
    "    else None,\n",
    "    axis=1,\n",
    ").dropna()\n",
    "\n",
    "rmsk[\"genoStart\"] = rmsk.apply(\n",
    "    lambda x: x[\"genoStart\"] - 1000 if x[\"strand\"] == \"-\" else x[\"genoStart\"], axis=1\n",
    ")\n",
    "rmsk[\"genoEnd\"] = rmsk.apply(\n",
    "    lambda x: x[\"genoEnd\"] + 1000 if x[\"strand\"] == \"+\" else x[\"genoEnd\"], axis=1\n",
    ")\n",
    "\n",
    "rmsk = rmsk.rename(\n",
    "    columns={\n",
    "        \"genoName\": \"Chromosome\",\n",
    "        \"genoStart\": \"Start\",\n",
    "        \"genoEnd\": \"End\",\n",
    "        \"strand\": \"Strand\",\n",
    "    }\n",
    ").loc[:, [\"Chromosome\", \"Start\", \"End\", \"Strand\"]]\n",
    "rmsk = rmsk.loc[(rmsk.Start >= 0) & (rmsk.End >= 0), :]\n",
    "\n",
    "rmsk[\"Name\"] = rmsk.index.values.astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to run `bedtools intersect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bam_intersect(bam, bed, view_args=\"\"):\n",
    "    \"\"\"\n",
    "    Intersect bam file with bedfile, optionally filter to bam first.\n",
    "    Return output as a dataframe\n",
    "    \"\"\"\n",
    "    cmd = f\"samtools view -b {bam} {view_args} | bedtools intersect -abam stdin -b {bed} -bed -wa -wb\"\n",
    "    p = Popen(cmd, shell=True, stdout=PIPE, stderr=DEVNULL)\n",
    "\n",
    "    names = [\n",
    "        \"Chromosome\",\n",
    "        \"Start\",\n",
    "        \"End\",\n",
    "        \"read_id\",\n",
    "        \"mapq\",\n",
    "        \"Strand\",\n",
    "        \"intersect_start\",\n",
    "        \"intersect_end\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"Flag\",\n",
    "        \"z\",\n",
    "        \"ChromosomeB\",\n",
    "        \"StartB\",\n",
    "        \"EndB\",\n",
    "        \"NameB\",\n",
    "        \"ScoreB\",\n",
    "        \"StrandB\",\n",
    "    ]\n",
    "    with StringIO(p.stdout.read().decode()) as bed:\n",
    "        df = pd.read_csv(bed, sep=\"\\t\", header=None, names=names)\n",
    "\n",
    "    df.drop([\"x\", \"y\", \"z\"], axis=1, inplace=True)\n",
    "    df[\"Flag\"] = df[\"Flag\"].str.rstrip(\",\").astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bed_intersect(peaks, annotation, **kwargs):\n",
    "    # compute reads per knrgl\n",
    "    # assert peaks.columns == [\"Chromosome\", \"Start\", \"End\", \"Strand\", \"nreads\", \"width\"], \"Incorrect columsn in peak file\"\n",
    "\n",
    "    # save bed files for bedtools\n",
    "    bed_names = [\"Chromosome\", \"Start\", \"End\", \"Name\", \"Score\", \"Strand\"]\n",
    "    peak_tmp = NamedTemporaryFile()\n",
    "    pr.PyRanges(peaks).to_bed(peak_tmp.name)\n",
    "    peak_names = bed_names + [c for c in peaks.columns if c not in bed_names]\n",
    "\n",
    "    annotation_tmp = NamedTemporaryFile()\n",
    "    pr.PyRanges(annotation[[\"Chromosome\", \"Start\", \"End\", \"Name\"]]).to_bed(\n",
    "        annotation_tmp.name\n",
    "    )\n",
    "    annotation_names = [f\"{c}b\" for c in bed_names] + [\n",
    "        c for c in annotation.columns if c not in bed_names\n",
    "    ]\n",
    "\n",
    "    cmd = f\"bedtools intersect -a {peak_tmp.name} -b {annotation_tmp.name} -wa -wb\"\n",
    "    p = Popen(cmd, shell=True, stdout=PIPE)\n",
    "    with StringIO(p.stdout.read().decode()) as bed:\n",
    "        df = pd.read_csv(\n",
    "            bed, sep=\"\\t\", header=None, names=peak_names + annotation_names\n",
    "        )\n",
    "    peak_tmp.close()\n",
    "    annotation_tmp.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define peak caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My custom peak caller\n",
    "# iterate over reads, cluster overlapping together\n",
    "# TODO: link r1 and r2 peaks\n",
    "\n",
    "\n",
    "# custom tuple to store reads in deque\n",
    "Read = namedtuple(\n",
    "    \"Read\",\n",
    "    [\n",
    "        \"query_name\" \"reference_name\",\n",
    "        \"reference_start\",\n",
    "        \"reference_end\",\n",
    "        \"is_read1\",\n",
    "        \"is_read2\",\n",
    "        \"is_forward\",\n",
    "        \"is_reverse\",\n",
    "        \"mapping_quality\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "class OverlapPeakCaller:\n",
    "    def __init__(self, bam: pysam.AlignmentFile) -> None:\n",
    "        self.bam = bam\n",
    "\n",
    "    def read_converter(self, read: pysam.AlignedSegment) -> Read:\n",
    "        \"convert pysam.AlignedSegment to namedtuple Read\"\n",
    "        return Read(\n",
    "            read.query_name,\n",
    "            read.reference_name,\n",
    "            read.reference_start,\n",
    "            read.reference_end,\n",
    "            read.is_read1,\n",
    "            read.is_read2,\n",
    "            read.is_forward,\n",
    "            read.is_reverse,\n",
    "            read.mapping_quality,\n",
    "        )\n",
    "\n",
    "    def make_peaks(self, reads, r1_bandwidth=0, min_mapq=60) -> deque:\n",
    "        \"\"\"\n",
    "        Iterate over reads, cluster overlapping reads together\n",
    "        Do this separately for read1_fwd, read1_rev, read2_fwd, read2_rev\n",
    "        \"\"\"\n",
    "\n",
    "        peak_groups = {\n",
    "            \"read1_fwd\": {\n",
    "                \"filter\": lambda x: x.is_read1 and x.is_forward,\n",
    "                \"peak\": deque(),\n",
    "                \"last_read\": None,\n",
    "            },\n",
    "            \"read1_rev\": {\n",
    "                \"filter\": lambda x: x.is_read1 and x.is_reverse,\n",
    "                \"peak\": deque(),\n",
    "                \"last_read\": None,\n",
    "            },\n",
    "            \"read2_fwd\": {\n",
    "                \"filter\": lambda x: x.is_read2 and x.is_forward,\n",
    "                \"peak\": deque(),\n",
    "                \"last_read\": None,\n",
    "            },\n",
    "            \"read2_rev\": {\n",
    "                \"filter\": lambda x: x.is_read2 and x.is_reverse,\n",
    "                \"peak\": deque(),\n",
    "                \"last_read\": None,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        for read in reads:\n",
    "            for rg, d in peak_groups.items():\n",
    "                # check if read is in read group\n",
    "                if not d[\"filter\"](read):\n",
    "                    continue\n",
    "                if d[\"last_read\"] is None:\n",
    "                    d[\"last_read\"] = read\n",
    "                    d[\"peak\"].append(read)\n",
    "                    continue\n",
    "\n",
    "                # if read is not within bandwith of last read, yield peak\n",
    "                if \"read2\" in rg:\n",
    "                    end = d[\"last_read\"].reference_end\n",
    "                else:\n",
    "                    end = d[\"last_read\"].reference_end + r1_bandwidth\n",
    "\n",
    "                if (read.reference_name != d[\"last_read\"].reference_name) or (\n",
    "                    read.reference_start > end\n",
    "                ):\n",
    "                    yield {\n",
    "                        \"read_group\": rg,\n",
    "                        \"Chromosome\": read.reference_name,\n",
    "                        \"Start\": d[\"peak\"][0].reference_start,\n",
    "                        \"End\": d[\"last_read\"].reference_end,\n",
    "                        \"Strand\": \"-\" if read.is_reverse else \"+\",\n",
    "                        \"nreads\": len(d[\"peak\"]),\n",
    "                    }\n",
    "                    # start new peak\n",
    "                    d[\"peak\"] = deque()\n",
    "\n",
    "                d[\"peak\"].append(read)\n",
    "                d[\"last_read\"] = read\n",
    "\n",
    "    def merge_peaks(self, peaks, bandwidth=0) -> dict:\n",
    "\n",
    "        last_peaks = {\n",
    "            \"read1_fwd\": None,\n",
    "            \"read2_fwd\": None,\n",
    "            \"read1_rev\": None,\n",
    "            \"read2_rev\": None,\n",
    "        }\n",
    "\n",
    "        for p in peaks:\n",
    "            # return unmatched peaks\n",
    "            if last_peaks[p.read_group] is not None:\n",
    "                yield {\n",
    "                    \"Chromosome\": last_peaks[p.read_group].Chromosome,\n",
    "                    \"Start\": last_peaks[p.read_group].Start,\n",
    "                    \"End\": last_peaks[p.read_group].End,\n",
    "                    \"Strand\": \"-\"\n",
    "                    if p.read_group == \"read1_fwd\" or \"read2_rev\"\n",
    "                    else \"+\",\n",
    "                    \"nreads\": p.nreads,\n",
    "                }\n",
    "\n",
    "            # if concordant peak is within bandwidth of last peak, merge and yield\n",
    "            for r1g, r2g in zip(\n",
    "                [\"read1_fwd\", \"read1_rev\", \"read2_rev\", \"read2_fwd\"],\n",
    "                [\"read2_rev\", \"read2_fwd\", \"read1_fwd\", \"read1_rev\"],\n",
    "            ):\n",
    "                if p.read_group == r1g and (\n",
    "                    last_peaks[r2g] is not None\n",
    "                    and p.Chromosome == last_peaks[r2g].Chromosome\n",
    "                    and p.Start < (last_peaks[r2g].End + bandwidth)\n",
    "                ):\n",
    "                    yield {\n",
    "                        \"Chromosome\": p.Chromosome,\n",
    "                        \"Start\": last_peaks[r2g].Start,\n",
    "                        \"End\": p.End,\n",
    "                        \"Strand\": \"-\" if r1g == \"read1_fwd\" or \"read2_rev\" else \"+\",\n",
    "                        \"nreads\": p.nreads + last_peaks[r2g].nreads,\n",
    "                    }\n",
    "\n",
    "                    # reset last peaks\n",
    "                    last_peaks[r2g] = None\n",
    "                    last_peaks[r1g] = None\n",
    "\n",
    "            last_peaks[p.read_group] = p\n",
    "\n",
    "    def run(self, r1_bandwidth=0, min_mapq=60):\n",
    "\n",
    "        # get the reads\n",
    "        reads = filter(\n",
    "            lambda x: x.is_mapped\n",
    "            and (not (x.is_secondary or x.is_supplementary))\n",
    "            and x.mapping_quality >= min_mapq,\n",
    "            self.bam.fetch(contig=\"chr1\"),\n",
    "        )\n",
    "        reads = map(self.read_converter, reads)\n",
    "\n",
    "        # get the peaks\n",
    "        peaks = [p for p in self.make_peaks(reads, r1_bandwidth, min_mapq)]\n",
    "\n",
    "        # sort peaks\n",
    "        peaks = (\n",
    "            pd.DataFrame.from_records(peaks)\n",
    "            .sort_values([\"Chromosome\", \"Start\", \"End\"])\n",
    "            .to_records()\n",
    "        )\n",
    "\n",
    "        # merge the peaks\n",
    "        merged = [p for p in self.merge_peaks(peaks, r1_bandwidth)]\n",
    "\n",
    "        # res[\"group\"] = res[\"read group\"].apply(lambda x: \"read1\" if \"read1\" in x else \"read2\")\n",
    "        # res[\"width\"] = res[\"End\"] - res[\"Start\"]\n",
    "\n",
    "        return peaks, merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define analysis helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analysis helper functions\n",
    "def annotate_peaks(peaks: pd.DataFrame, annotation: pd.DataFrame, name: str):\n",
    "    \"\"\"\n",
    "    Annotate peaks that overlap entries in annotation\n",
    "    \"\"\"\n",
    "\n",
    "    # intersect peaks with annotation\n",
    "    intersection = bed_intersect(peaks, annotation)\n",
    "    intersection = (\n",
    "        intersection[[\"Chromosome\", \"Start\", \"End\", \"Nameb\"]]\n",
    "        .set_index([\"Chromosome\", \"Start\", \"End\"])\n",
    "        .rename(columns={\"Nameb\": f\"{name} ID\"})\n",
    "    )\n",
    "\n",
    "    # annotate peaks\n",
    "    peaks = peaks.join(intersection, on=[\"Chromosome\", \"Start\", \"End\"], how=\"left\")\n",
    "    peaks[f\"{name} cov\"] = len(intersection.groupby([f\"{name} ID\"]).size())\n",
    "    peaks[name] = len(annotation)\n",
    "\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plotting helper funciton\n",
    "def peak_cdf_plot(df, hue_col=\"indv\"):\n",
    "    # melt df\n",
    "    df = df.melt(\n",
    "        id_vars=[\"Chromosome\", \"Start\", \"End\", hue_col, \"group\"],\n",
    "        value_vars=[\"nreads\", \"width\"],\n",
    "        var_name=\"metric\",\n",
    "        value_name=\"value\",\n",
    "    )\n",
    "\n",
    "    # create facetgrid\n",
    "    g = sns.FacetGrid(\n",
    "        df,\n",
    "        col=\"metric\",\n",
    "        row=\"group\",\n",
    "        hue=hue_col,\n",
    "        sharex=False,\n",
    "        sharey=True,\n",
    "        height=4,\n",
    "        aspect=1,\n",
    "    )\n",
    "    g.map_dataframe(sns.ecdfplot, x=\"value\", stat=\"proportion\")\n",
    "\n",
    "    # add legend\n",
    "    g.add_legend()\n",
    "\n",
    "    # if nreads, set x axis to log scale\n",
    "    g.axes[0, 0].set_xscale(\"log\")\n",
    "    g.axes[1, 0].set_xscale(\"log\")\n",
    "\n",
    "    # if width, set xlim to 0\n",
    "    g.axes[0, 1].set_xlim(0, None)\n",
    "    g.axes[1, 1].set_xlim(0, None)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find knrgl bed and bulk BAMs for each individual\n",
    "individuals = pd.read_csv(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/config/bulk_donors.tsv\", sep=\"\\t\"\n",
    ")[\"donor_id\"].values.astype(str)\n",
    "indv_data = {i: {\"knrgl\": None, \"bulk\": None} for i in individuals}\n",
    "\n",
    "for bulk in Path(\"/iblm/netapp/data4/mcuoco/sz_slavseq/results/align/\").rglob(\n",
    "    \"*/gDNA_usd*.sorted.bam\"\n",
    "):\n",
    "    if bulk.parts[-2] in individuals:\n",
    "        indv_data[bulk.parts[-2]][\"bulk\"] = str(bulk)\n",
    "\n",
    "for knrgl in Path(\"/iblm/netapp/data4/mcuoco/sz_slavseq/resources/\").rglob(\n",
    "    \"*_insertions.bed\"\n",
    "):\n",
    "    if knrgl.name.split(\"_\")[0] in individuals:\n",
    "        indv_data[knrgl.name.split(\"_\")[0]][\"knrgl\"] = str(knrgl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `bandwith` and `min_mapq` parameters\n",
    "\n",
    "Use individual 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test peak caller\n",
    "bam = pysam.AlignmentFile(indv_data[\"27\"][\"bulk\"], \"rb\")\n",
    "peaks, merged = OverlapPeakCaller(bam).run(r1_bandwidth=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = [250, 500, 1000, 2000]\n",
    "min_mapqs = [5, 10, 20, 30, 40, 50, 60]\n",
    "data = indv_data[\"27\"]\n",
    "\n",
    "total_reads = int(pysam.view(\"-c\", data[\"bulk\"]).rstrip(\"\\n\"))\n",
    "bam = pysam.AlignmentFile(data[\"bulk\"], \"rb\")\n",
    "knrgl = read_knrgl(data[\"knrgl\"])\n",
    "knrgl[\"Name\"] = knrgl.index.values.astype(str)  # give each knrgl a unique ID\n",
    "\n",
    "res = []\n",
    "pc = OverlapPeakCaller(bam)\n",
    "for bw, mq in tqdm(\n",
    "    product(bandwidths, min_mapqs), total=(len(bandwidths) * len(min_mapqs))\n",
    "):\n",
    "    # call peaks\n",
    "    peaks = pc.run(r1_bandwidth=bw, min_mapq=mq)\n",
    "\n",
    "    # annotate\n",
    "    peaks = annotate_peaks(peaks, knrgl, \"knrgl\")\n",
    "    peaks = annotate_peaks(peaks, rmsk, \"rmsk\")\n",
    "\n",
    "    peaks[\"bw\"] = bw\n",
    "    peaks[\"min_mapq\"] = mq\n",
    "\n",
    "    # collect results\n",
    "    res.append(peaks)\n",
    "\n",
    "res = pd.concat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"frac knrgl cov\"] = res[\"knrgl cov\"] / res[\"knrgl\"]\n",
    "res[\"frac rmsk cov\"] = res[\"rmsk cov\"] / res[\"rmsk\"]\n",
    "plot_df = (\n",
    "    res.groupby([\"bw\", \"min_mapq\", \"frac knrgl cov\", \"frac rmsk cov\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total peaks\"})\n",
    "    .melt(\n",
    "        id_vars=[\"bw\", \"min_mapq\"],\n",
    "        value_vars=[\"total peaks\", \"frac knrgl cov\", \"frac rmsk cov\"],\n",
    "        value_name=\"value\",\n",
    "        var_name=\"var\",\n",
    "    )\n",
    ")\n",
    "\n",
    "sns.catplot(\n",
    "    data=plot_df,\n",
    "    x=\"bw\",\n",
    "    y=\"value\",\n",
    "    col=\"var\",\n",
    "    hue=\"min_mapq\",\n",
    "    kind=\"bar\",\n",
    "    palette=\"Set2\",\n",
    "    sharey=False,\n",
    "    dodge=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_cdf_plot(res, hue_col=\"min_mapq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_cdf_plot(res[res[\"knrgl ID\"].notnull()], hue_col=\"min_mapq\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze across individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for ind, data in tqdm(indv_data.items(), total=len(indv_data)):\n",
    "    # total_reads = int(pysam.view(\"-c\", data[\"bulk\"], \"chr22\").rstrip(\"\\n\")) # subset to chr22 (for testing)\n",
    "    total = int(pysam.view(\"-c\", data[\"bulk\"]).rstrip(\"\\n\"))\n",
    "    bam = pysam.AlignmentFile(data[\"bulk\"], \"rb\")\n",
    "\n",
    "    # call peaks\n",
    "    peaks = OverlapPeakCaller(bam).call_peaks()\n",
    "\n",
    "    ## Compare with knrgl and rmsk\n",
    "    knrgl = read_knrgl(data[\"knrgl\"])\n",
    "    knrgl[\"Name\"] = knrgl.index.values.astype(str)  # give each knrgl a unique ID\n",
    "    peaks = annotate_peaks(peaks, knrgl, \"KNRGL\")\n",
    "    peaks = annotate_peaks(peaks, rmsk, \"RMSL\")\n",
    "\n",
    "    # collect results\n",
    "    peaks[\"indv\"] = int(ind)\n",
    "    peaks[\"total alignments\"] = total\n",
    "    res.append(peaks)\n",
    "\n",
    "res = pd.concat(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metadata\n",
    "meta = pd.read_csv(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/config/slavseq_metadata.tsv\", sep=\"\\t\"\n",
    ")\n",
    "meta = meta[~meta[\"TISSUE_ID\"].isin([\"CommonBrain\"])]\n",
    "meta[\"indv\"] = meta[\"TISSUE_ID\"].str[3:].astype(int)\n",
    "meta.set_index(\"indv\", inplace=True)\n",
    "\n",
    "# keep columns of interest\n",
    "meta = meta[[\"RACE\", \"AGE\", \"DIAGNOSIS\"]].drop_duplicates()\n",
    "\n",
    "# join with res\n",
    "res = res.join(meta, on=\"indv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "res[\"frac knrgl cov\"] = res[\"knrgl cov\"] / res[\"knrgl\"]\n",
    "res[\"frac rmsk cov\"] = res[\"rmsk cov\"] / res[\"rmsk\"]\n",
    "plot_df = (\n",
    "    res.groupby(\n",
    "        [\n",
    "            \"indv\",\n",
    "            \"total alignments\",\n",
    "            \"frac knrgl cov\",\n",
    "            \"frac rmsk cov\",\n",
    "            \"AGE\",\n",
    "            \"RACE\",\n",
    "            \"DIAGNOSIS\",\n",
    "        ]\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"total peaks\"})\n",
    "    .melt(\n",
    "        id_vars=[\"indv\", \"AGE\", \"RACE\", \"DIAGNOSIS\"],\n",
    "        value_vars=[\n",
    "            \"total alignments\",\n",
    "            \"total peaks\",\n",
    "            \"frac knrgl cov\",\n",
    "            \"frac rmsk cov\",\n",
    "        ],\n",
    "        value_name=\"value\",\n",
    "        var_name=\"var\",\n",
    "    )\n",
    ")\n",
    "\n",
    "sns.catplot(\n",
    "    data=plot_df,\n",
    "    x=\"indv\",\n",
    "    y=\"value\",\n",
    "    hue=\"indv\",\n",
    "    col=\"var\",\n",
    "    kind=\"bar\",\n",
    "    palette=\"Set2\",\n",
    "    sharey=False,\n",
    "    dodge=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = peak_cdf_plot(res)\n",
    "g.fig.subplots_adjust(top=0.90)\n",
    "g.fig.suptitle(\"All peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = peak_cdf_plot(res[res[\"KNRGL_ID\"].notnull()])\n",
    "g.fig.subplots_adjust(top=0.90)\n",
    "g.fig.suptitle(\"Peaks overlapping KNRGL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = peak_cdf_plot(res[res[\"RMSK_ID\"].notnull()])\n",
    "g.fig.subplots_adjust(top=0.90)\n",
    "g.fig.suptitle(\"Peaks overlapping RMSK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many peaks of each type per knrgl insertion?\n",
    "count = (\n",
    "    res[[\"indv\", \"KNRGL_ID\", \"read group\", \"level_1\"]]\n",
    "    .pivot_table(\n",
    "        index=[\"indv\", \"KNRGL_ID\"],\n",
    "        columns=\"read group\",\n",
    "        values=\"level_1\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "count[[\"read1_fwd\", \"read1_rev\", \"read2_fwd\", \"read2_rev\"]].max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many peaks of each type per knrgl insertion?\n",
    "count = (\n",
    "    res[[\"indv\", \"KNRGL_ID\", \"read group\", \"level_1\"]]\n",
    "    .pivot_table(\n",
    "        index=[\"indv\", \"KNRGL_ID\"],\n",
    "        columns=\"read group\",\n",
    "        values=\"level_1\",\n",
    "        aggfunc=\"count\",\n",
    "    )\n",
    "    .fillna(0)\n",
    ")\n",
    "count[[\"read1_fwd\", \"read1_rev\", \"read2_fwd\", \"read2_rev\"]].sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the peaks with nreads = 1? Should they be grouped with other peaks or are they artifactual alignments?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
