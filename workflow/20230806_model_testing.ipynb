{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# parallelization\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# data science / ML\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from flaml import AutoML\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# my own modules\n",
    "from scripts.pyslavseq.model_selection import SampleChrSplitter, Model, parse_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and save data, takes a long time to run\n",
    "data = []\n",
    "for f in Path(\"../results/model/get_labels/\").rglob(\"*nonrefonly.pqt\"):\n",
    "    print(f\"Reading {f}\")\n",
    "    ddata = pq.read_table(f).to_pandas()\n",
    "    ddata = ddata.loc[ddata.rpm >= 2, :]\n",
    "    if \"bulk_peaks\" in ddata.columns:\n",
    "        ddata = ddata.drop(columns=[\"bulk_peaks\", \"bulk_peaks_id\"]).drop_duplicates()\n",
    "    # convert float32 to float16\n",
    "    for c in ddata.columns:\n",
    "        if (ddata[c].dtype == \"float32\") and (c != \"rpm\"):\n",
    "            ddata[c] = ddata[c].astype(\"float16\")\n",
    "            assert not np.isinf(ddata[c]).any(), f\"{c} column contains inf values\"\n",
    "            assert not ddata[c].isna().any(), f\"{c} column contains nan values\"\n",
    "    data.append(ddata)\n",
    "\n",
    "data = pd.concat(data)\n",
    "assert (\n",
    "    data.shape[0]\n",
    "    == data[[\"Chromosome\", \"Start\", \"End\", \"cell_id\"]].drop_duplicates().shape[0]\n",
    "), \"some rows have been duplicated during labeling!\"\n",
    "\n",
    "# save\n",
    "data.to_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data.donor_id.nunique() == 38, \"Wrong number of donors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata\n",
    "meta = pd.read_csv(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/config/slavseq_metadata.tsv\", sep=\"\\t\"\n",
    ")\n",
    "meta.columns = [col.lower() for col in meta.columns]\n",
    "donors = pd.read_csv(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/config/all_donors.tsv\", sep=\"\\t\"\n",
    ")\n",
    "cells = pd.read_csv(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/config/all_samples.tsv\", sep=\"\\t\"\n",
    ")\n",
    "cells = pd.merge(cells, donors, on=\"donor_id\", how=\"left\")\n",
    "cells = pd.merge(\n",
    "    cells, meta[[\"tissue_id\", \"sequencing\", \"region\"]], on=\"tissue_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize window reads by labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ecdf(data):\n",
    "\n",
    "    sorted_data = np.sort(data)[::-1]\n",
    "    y_values = np.arange(1, len(data) + 1)\n",
    "\n",
    "    return sorted_data, y_values\n",
    "\n",
    "\n",
    "def label_ecdf(data, label):\n",
    "    assert label in data.columns, f\"{label} not in data\"\n",
    "    assert data[label].dtype == bool, f\"{label} must be boolean\"\n",
    "\n",
    "    x_pos, y_pos = compute_ecdf(data[data[label]][\"rpm\"].values)\n",
    "    x_neg, y_neg = compute_ecdf(data[~data[label]][\"rpm\"].values)\n",
    "\n",
    "    return {\"pos\": (x_pos, y_pos), \"neg\": (x_neg, y_neg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axs) = plt.subplots(\n",
    "    data.donor_id.nunique(), 3, figsize=(17, 6 * data.donor_id.nunique())\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "for (d, ddata), ax1, ax2, ax3 in zip(\n",
    "    data.groupby(\"donor_id\"), axs[:, 0], axs[:, 1], axs[:, 2]\n",
    "):\n",
    "    for ax, label in zip([ax1, ax2, ax3], [\"xtea\", \"xtea_1kb_3end\", \"bulk_peaks\"]):\n",
    "        print(f\"Running donor {d}, label {label}\")\n",
    "        ecdf = np.array(\n",
    "            Parallel(n_jobs=4, verbose=2)(\n",
    "                delayed(label_ecdf)(df, label) for _, df in ddata.groupby(\"cell_id\")\n",
    "            )\n",
    "        )\n",
    "        for e in ecdf:\n",
    "            ax.plot(e[\"pos\"][0], e[\"pos\"][1], c=sns.color_palette()[0], alpha=0.5)\n",
    "            ax.plot(e[\"neg\"][0], e[\"neg\"][1], c=sns.color_palette()[1], alpha=0.5)\n",
    "\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"RPM\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend([\"True\", \"False\"]).set_title(label)\n",
    "        ax.set_title(f\"Donor {d}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "features = []\n",
    "keys = [\"_mean\", \"frac\", \"gini\", \"bias\"]\n",
    "for c in data.columns:\n",
    "    if (\"_score\" in c) or (\"_length\" in c):\n",
    "        if \"_normed\" not in c:\n",
    "            continue\n",
    "    for k in keys:\n",
    "        if k in c:\n",
    "            features.append(c)\n",
    "features.append(\"rpm\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# define the classifier\n",
    "clf = AutoML(\n",
    "    task=\"classification\",\n",
    "    estimator_list=[\"xgboost\"],\n",
    "    early_stop=True,\n",
    "    eval_method=\"cv\",\n",
    "    time_budget=120,  # time budget in seconds\n",
    "    metric=\"ap\",\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    "    verbose=4,\n",
    ")\n",
    "\n",
    "# setup outdir\n",
    "Path(\"model_logs\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def cv(\n",
    "    clf: AutoML, data: pd.DataFrame, features: list, label_col: str, rpm_filter: int\n",
    "):\n",
    "\n",
    "    # initialize my custom model class\n",
    "    mdl = Model(\n",
    "        data=data, features=features, label_col=label_col, rpm_filter=rpm_filter\n",
    "    )\n",
    "\n",
    "    # initialize splitter\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    result = []\n",
    "    for i, (train_idx, test_idx) in enumerate(\n",
    "        skf.split(mdl.data, mdl.data[mdl.label_col])\n",
    "    ):\n",
    "        print(f\"Fold {i+1}\")\n",
    "        train_metrics, test_metrics, model_metrics = mdl.fit(\n",
    "            train_idx,\n",
    "            test_idx,\n",
    "            clf,\n",
    "            sample_col=\"cell_id\",\n",
    "            n_chr_splits=2,\n",
    "            n_sample_splits=2,\n",
    "        )\n",
    "        train_metrics[\"fold\"], test_metrics[\"fold\"] = i + 1, i + 1\n",
    "        train_metrics.update(model_metrics)\n",
    "        test_metrics.update(model_metrics)\n",
    "        result.append(train_metrics)\n",
    "        result.append(test_metrics)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# define the data\n",
    "ddata = data[data.donor_id == \"CommonBrain\"]\n",
    "out = cv(clf, ddata, features, label_col=\"xtea_1kb_3end\", rpm_filter=5)\n",
    "out = pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    out.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def cv(\n",
    "    clf: AutoML, data: pd.DataFrame, features: list, label_col: str, rpm_filter: int\n",
    "):\n",
    "\n",
    "    # initialize my custom model class\n",
    "    mdl = Model(\n",
    "        data=data, features=features, label_col=label_col, rpm_filter=rpm_filter\n",
    "    )\n",
    "\n",
    "    # initialize splitter\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5)\n",
    "\n",
    "    result = []\n",
    "    for i, (train_idx, test_idx) in enumerate(\n",
    "        sgkf.split(mdl.data, mdl.data[mdl.label_col], groups=mdl.data[\"cell_id\"])\n",
    "    ):\n",
    "        print(f\"Fold {i+1}\")\n",
    "        train_metrics, test_metrics, model_metrics = mdl.fit(\n",
    "            train_idx,\n",
    "            test_idx,\n",
    "            clf,\n",
    "            sample_col=\"cell_id\",\n",
    "            n_chr_splits=2,\n",
    "            n_sample_splits=2,\n",
    "        )\n",
    "        train_metrics[\"fold\"], test_metrics[\"fold\"] = i + 1, i + 1\n",
    "        train_metrics.update(model_metrics)\n",
    "        test_metrics.update(model_metrics)\n",
    "        result.append(train_metrics)\n",
    "        result.append(test_metrics)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# define the data\n",
    "ddata = data[data.donor_id == \"CommonBrain\"]\n",
    "out = cv(clf, ddata, features, label_col=\"xtea_1kb_3end\", rpm_filter=5)\n",
    "out = pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    out.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by chromosome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize sample_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(scale_pos_weight=int):\n",
    "    # define the data\n",
    "    ddata = data[data.donor_id == \"CommonBrain\"]\n",
    "\n",
    "    # add scale_pos_weight to settings\n",
    "    clf._settings[\"scale_pos_weight\"] = scale_pos_weight\n",
    "\n",
    "    mdl = Model(\n",
    "        clf=clf,\n",
    "        data=ddata,\n",
    "        features=features,\n",
    "        label_col=\"xtea_1kb_3end\",\n",
    "        rpm_filter=5,\n",
    "        outfile=f\"model_logs/CommonBrain_weight{scale_pos_weight}.log\",\n",
    "    )\n",
    "    mdl.cv(n_splits=5)\n",
    "    out = mdl.get_results()\n",
    "    out[\"scale_pos_weight\"] = scale_pos_weight\n",
    "    return out\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=6, verbose=2)(\n",
    "    delayed(run_cv)(w) for w in [1, 10, 20, 50, 100, 200]\n",
    ")\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    results.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    row=\"scale_pos_weight\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On all data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 30 min\n",
    "mdl = Model(\n",
    "    clf=clf,\n",
    "    data=data,\n",
    "    features=features,\n",
    "    label_col=\"xtea_1kb_3end\",\n",
    "    rpm_filter=5,\n",
    "    outfile=\"model_logs/all_120_budget_41_posweight_no_static_concurrent.log\",\n",
    ")\n",
    "mdl.cv(n_splits=5)\n",
    "results = mdl.get_results()\n",
    "sns.relplot(\n",
    "    results.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On seq platforms separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(platform: str, cells: list):\n",
    "    # define the data\n",
    "    ddata = data[data.cell_id.isin(cells)]\n",
    "    mdl = Model(\n",
    "        clf=clf,\n",
    "        data=ddata,\n",
    "        features=features,\n",
    "        label_col=\"xtea_1kb_3end\",\n",
    "        rpm_filter=5,\n",
    "        outfile=f\"model_logs/{platform}.log\",\n",
    "    )\n",
    "    mdl.cv(n_splits=5)\n",
    "    out = mdl.get_results()\n",
    "    out[\"platform\"] = platform\n",
    "    return out\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=2, verbose=2)(\n",
    "    delayed(run_cv)(platform, cells[cells[\"sequencing\"] == platform].sample_id.unique())\n",
    "    for platform in [\"NOVASEQ\", \"HISEQ\"]\n",
    ")\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    results.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    row=\"platform\",\n",
    "    kind=\"line\",\n",
    ")\n",
    "g.set(xlim=(0, 1), ylim=(0, 1), xlabel=\"Adjusted locus recall\", ylabel=\"Precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On each tissue separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(tissue_id: str, cells: list):\n",
    "    # define the data\n",
    "    ddata = data[data.cell_id.isin(cells)]\n",
    "    mdl = Model(\n",
    "        clf=clf,\n",
    "        data=ddata,\n",
    "        features=features,\n",
    "        label_col=\"xtea_1kb_3end\",\n",
    "        rpm_filter=5,\n",
    "        outfile=f\"model_logs/{tissue_id}_scale_pos.log\",\n",
    "    )\n",
    "    mdl.cv(n_splits=5)\n",
    "    out = mdl.get_results()\n",
    "    out[\"tissue_id\"] = tissue_id\n",
    "    return out\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=32, verbose=2)(\n",
    "    delayed(run_cv)(tissue_id, df.sample_id.values)\n",
    "    for tissue_id, df in cells.groupby(\"tissue_id\")\n",
    ")\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for file in Path(\"model_logs\").rglob(\"*.log\"):\n",
    "    if file.stem not in cells.tissue_id.unique():\n",
    "        continue\n",
    "    with open(file) as f:\n",
    "        log = json.load(f)\n",
    "    log = parse_log(log)\n",
    "    log[\"tissue_id\"] = file.stem.rstrip(\"_scale_pos\")\n",
    "    results.append(log)\n",
    "results = pd.concat(results)\n",
    "\n",
    "tissues = (\n",
    "    cells.drop(columns=[\"sample_id\", \"R1\", \"R2\", \"xtea\"])\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"tissue_id\")\n",
    ")\n",
    "results = results.set_index(\"tissue_id\").join(tissues, how=\"left\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    results[results[\"stage\"] == \"test\"]\n",
    "    .explode([\"precision\", \"adjusted_locus_recall\"])\n",
    "    .groupby(\n",
    "        [\"tissue_id\", \"stage\", \"sequencing\", \"race\", \"diagnosis\", \"age\", \"region\"]\n",
    "    )[[\"precision\", \"adjusted_locus_recall\", \"total_loci_train\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knrgl in training set\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    df, x=\"adjusted_locus_recall\", y=\"precision\", hue=\"total_loci_train\", ax=ax1\n",
    ")\n",
    "g.set(xlabel=\"Mean Adjusted Locus Recall\", ylabel=\"Mean Precision\")\n",
    "g.set_title(\"Mean XGBoost performance across 5-fold Chromosome CV for each tissue\")\n",
    "g.legend(title=\"Mean KNRGL in training set\")\n",
    "\n",
    "g = sns.scatterplot(df, y=\"precision\", x=\"total_loci_train\", ax=ax2)\n",
    "g.set(xlabel=\"Mean KNRGL in training set\", ylabel=\"Mean Precision\")\n",
    "\n",
    "# add correlation coefficient from scipy\n",
    "r, p = pearsonr(df[\"total_loci_train\"], df[\"precision\"])\n",
    "g.text(0.05, 0.15, f\"Pearson: r = {r:.2f}, p = {p:.2e}\", transform=g.transAxes)\n",
    "r, p = spearmanr(df[\"total_loci_train\"], df[\"precision\"])\n",
    "g.text(0.05, 0.1, f\"Spearman: r = {r:.2f}, p = {p:.2e}\", transform=g.transAxes)\n",
    "\n",
    "\n",
    "g = sns.scatterplot(df, y=\"adjusted_locus_recall\", x=\"total_loci_train\", ax=ax3)\n",
    "g.set(xlabel=\"Mean KNRGL in training set\", ylabel=\"Mean Adjusted Locus Recall\")\n",
    "\n",
    "# add correlation coefficient from scipy\n",
    "r, p = pearsonr(df[\"total_loci_train\"], df[\"adjusted_locus_recall\"])\n",
    "g.text(0.05, 0.15, f\"Pearson: r = {r:.2f}, p = {p:.2e}\", transform=g.transAxes)\n",
    "r, p = spearmanr(df[\"total_loci_train\"], df[\"adjusted_locus_recall\"])\n",
    "g.text(0.05, 0.1, f\"Spearman: r = {r:.2f}, p = {p:.2e}\", transform=g.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"precision\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CommonBrain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    results[results[\"tissue_id\"] == \"CommonBrain\"].explode(\n",
    "        [\"precision\", \"adjusted_locus_recall\"]\n",
    "    ),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average KNRGL coverage per tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = []\n",
    "for f in Path(\"../results/qc/l1_coverage\").rglob(\"*/*xtea_1kb_3end.r1.txt\"):\n",
    "    res = {}\n",
    "    df = pd.read_csv(\n",
    "        f,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=[\n",
    "            \"Chromosome\",\n",
    "            \"Start\",\n",
    "            \"End\",\n",
    "            \"Name\",\n",
    "            \"Score\",\n",
    "            \"Strand\",\n",
    "            \"n_reads\",\n",
    "            \"n_bases_overlapped\",\n",
    "            \"l1_length\",\n",
    "            \"frac_overlap\",\n",
    "        ],\n",
    "    )\n",
    "    res[\"knrgl_zeros\"] = df[df[\"n_reads\"] == 0].shape[0]\n",
    "    res[\"cell_id\"] = f.stem.replace(\".xtea_1kb_3end.r1\", \"\")\n",
    "    res[\"donor_id\"] = f.parent.name\n",
    "    res[\"region\"] = \"HIPPO\" if \"USH\" in res[\"cell_id\"].upper() else \"DLPFC\"\n",
    "    res[\"total_knrgl\"] = df.shape[0]\n",
    "    res[\"knrgl_covered\"] = res[\"total_knrgl\"] - res[\"knrgl_zeros\"]\n",
    "    res[\"frac_missing\"] = res[\"knrgl_zeros\"] / res[\"total_knrgl\"]\n",
    "    cov.append(res)\n",
    "\n",
    "cov = pd.DataFrame(cov)\n",
    "cov = (\n",
    "    cov.groupby([\"donor_id\", \"total_knrgl\", \"region\"])[\n",
    "        [\"knrgl_zeros\", \"frac_missing\", \"knrgl_covered\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .sort_values(by=[\"knrgl_covered\"], ascending=False)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total_knrgl per donor\n",
    "g = sns.barplot(\n",
    "    y=\"donor_id\", x=\"total_knrgl\", data=cov.sort_values(\"total_knrgl\"), color=\"gray\"\n",
    ")\n",
    "g.set(xlabel=\"Total KNRGL detected from WGS\", ylabel=\"Donor ID\")\n",
    "# make figure size 10 x 10\n",
    "g.figure.set_size_inches(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from cells\n",
    "results = (\n",
    "    results.set_index([\"donor_id\", \"region\"])\n",
    "    .join(cov.set_index([\"donor_id\", \"region\"]), how=\"left\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    results[results[\"stage\"] == \"test\"]\n",
    "    .explode([\"precision\", \"adjusted_locus_recall\"])\n",
    "    .groupby(\n",
    "        [\n",
    "            \"tissue_id\",\n",
    "            \"stage\",\n",
    "            \"region\",\n",
    "            \"diagnosis\",\n",
    "            \"race\",\n",
    "            \"sex\",\n",
    "            \"sequencing\",\n",
    "            \"donor_id\",\n",
    "            \"knrgl_covered\",\n",
    "            \"total_knrgl\",\n",
    "            \"libd_id\",\n",
    "        ]\n",
    "    )[[\"precision\", \"adjusted_locus_recall\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(df, x=\"adjusted_locus_recall\", y=\"precision\")\n",
    "g.set(xlabel=\"Mean Adjusted Locus Recall\", ylabel=\"Mean Precision\")\n",
    "g.set_title(\"Mean XGBoost performance across 5-fold Chromosome CV for each tissue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(df, x=\"adjusted_locus_recall\", y=\"precision\", hue=\"knrgl_covered\")\n",
    "g.set(xlabel=\"Mean Adjusted Locus Recall\", ylabel=\"Mean Precision\")\n",
    "g.set_title(\"Mean XGBoost performance across 5-fold Chromosome CV for each tissue\")\n",
    "g.legend(\n",
    "    title=\"Mean KNRGL Covered / cell\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=2,\n",
    "    borderaxespad=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.scatterplot(df, x=\"adjusted_locus_recall\", y=\"precision\", hue=\"total_knrgl\")\n",
    "g.set(xlabel=\"Mean Adjusted Locus Recall\", ylabel=\"Mean Precision\")\n",
    "g.set_title(\"Mean XGBoost performance across 5-fold Chromosome CV for each tissue\")\n",
    "g.legend(\n",
    "    title=\"Total KNRGL detected from WGS\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=2,\n",
    "    borderaxespad=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly express\n",
    "import plotly.express as px\n",
    "\n",
    "px.scatter(\n",
    "    df,\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    color=\"knrgl_covered\",\n",
    "    hover_name=\"tissue_id\",\n",
    "    hover_data=[\n",
    "        \"tissue_id\",\n",
    "        \"knrgl_covered\",\n",
    "        \"adjusted_locus_recall\",\n",
    "        \"precision\",\n",
    "        \"libd_id\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does performance change with different cutoffs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def cv(\n",
    "    clf: AutoML, data: pd.DataFrame, features: list, label_col: str, rpm_filter: int\n",
    "):\n",
    "\n",
    "    # initialize my custom model class\n",
    "    mdl = Model(\n",
    "        data=data, features=features, label_col=label_col, rpm_filter=rpm_filter\n",
    "    )\n",
    "\n",
    "    # initialize splitter\n",
    "    splitter = SampleChrSplitter(\n",
    "        X=mdl.data,\n",
    "        y=mdl.data[label_col],\n",
    "        sample_col=\"cell_id\",\n",
    "        n_chr_splits=4,\n",
    "        n_sample_splits=2,\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split(mdl.data)):\n",
    "        print(f\"Fold {i+1}\")\n",
    "        train_metrics, test_metrics, model_metrics = mdl.fit(\n",
    "            train_idx,\n",
    "            test_idx,\n",
    "            clf,\n",
    "            sample_col=\"cell_id\",\n",
    "            n_chr_splits=4,\n",
    "            n_sample_splits=2,\n",
    "        )\n",
    "        train_metrics[\"fold\"], test_metrics[\"fold\"] = i + 1, i + 1\n",
    "        train_metrics.update(model_metrics)\n",
    "        test_metrics.update(model_metrics)\n",
    "        result.append(train_metrics)\n",
    "        result.append(test_metrics)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv(\n",
    "    clf=clf,\n",
    "    data=data,\n",
    "    features=features,\n",
    "    label_col=\"xtea_1kb_3end\",\n",
    "    rpm_filter=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    pd.DataFrame(out).explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"fold\",\n",
    "    col=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for rpm in [2, 3, 5, 7, 10]:\n",
    "    print(f\"Running {rpm} RPM\")\n",
    "\n",
    "    out = cv(\n",
    "        clf=clf,\n",
    "        data=data,\n",
    "        features=features,\n",
    "        label_col=\"xtea_1kb_3end\",\n",
    "        rpm_filter=rpm,\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame(out)\n",
    "    out[\"rpm\"] = rpm\n",
    "    result.append(out)\n",
    "\n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    result.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"rpm\",\n",
    "    col=\"fold\",\n",
    "    row=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of feature importances\n",
    "fresult = (\n",
    "    result.explode([\"features\", \"feature_importances\"])\n",
    "    .groupby([\"rpm\", \"features\"])\n",
    "    .agg({\"feature_importances\": \"mean\"})\n",
    "    .reset_index()\n",
    "    .pivot(index=\"features\", columns=\"rpm\", values=\"feature_importances\")\n",
    ")\n",
    "sns.heatmap(fresult.astype(float)).set(xlabel=\"rpm filter\", ylabel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for rpm in [2, 5, 10, 20, 50, 100]:\n",
    "    print(f\"Running {rpm} RPM\")\n",
    "\n",
    "    out = cv(\n",
    "        clf=clf,\n",
    "        data=data,\n",
    "        features=features,\n",
    "        label_col=\"xtea_1kb_3end\",\n",
    "        rpm_filter=rpm,\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame(out)\n",
    "    out[\"rpm\"] = rpm\n",
    "    result.append(out)\n",
    "\n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    result.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"rpm\",\n",
    "    col=\"fold\",\n",
    "    row=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of feature importances\n",
    "fresult = (\n",
    "    result.explode([\"features\", \"feature_importances\"])\n",
    "    .groupby([\"rpm\", \"features\"])\n",
    "    .agg({\"feature_importances\": \"mean\"})\n",
    "    .reset_index()\n",
    "    .pivot(index=\"features\", columns=\"rpm\", values=\"feature_importances\")\n",
    ")\n",
    "sns.heatmap(fresult.astype(float)).set(xlabel=\"rpm filter\", ylabel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does performance change with different metrics for optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features\n",
    "features = []\n",
    "keys = [\"_mean\", \"frac\", \"gini\", \"bias\"]\n",
    "for c in data.columns:\n",
    "    if (\"_score\" in c) and (\"_normed\" not in c):\n",
    "        continue\n",
    "    for k in keys:\n",
    "        if k in c:\n",
    "            features.append(c)\n",
    "features.append(\"rpm\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# define the classifier\n",
    "clf = AutoML(\n",
    "    task=\"classification\",\n",
    "    estimator_list=[\"xgboost\"],\n",
    "    early_stop=True,\n",
    "    eval_method=\"cv\",\n",
    "    time_budget=120,  # time budget in seconds\n",
    "    verbose=0,\n",
    "    metric=\"ap\",\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def cv(\n",
    "    clf: AutoML, data: pd.DataFrame, features: list, label_col: str, rpm_filter: int\n",
    "):\n",
    "\n",
    "    # initialize my custom model class\n",
    "    mdl = Model(\n",
    "        data=data, features=features, label_col=label_col, rpm_filter=rpm_filter\n",
    "    )\n",
    "\n",
    "    # initialize splitter\n",
    "    splitter = SampleChrSplitter(\n",
    "        X=mdl.data,\n",
    "        y=mdl.data[label_col],\n",
    "        sample_col=\"cell_id\",\n",
    "        n_chr_splits=4,\n",
    "        n_sample_splits=4,\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split(mdl.data)):\n",
    "        print(f\"Fold {i+1}\")\n",
    "        train_metrics, test_metrics, model_metrics = mdl.fit(\n",
    "            train_idx,\n",
    "            test_idx,\n",
    "            clf,\n",
    "            sample_col=\"cell_id\",\n",
    "            n_chr_splits=2,\n",
    "            n_sample_splits=2,\n",
    "        )\n",
    "        train_metrics[\"fold\"], test_metrics[\"fold\"] = i + 1, i + 1\n",
    "        train_metrics.update(model_metrics)\n",
    "        test_metrics.update(model_metrics)\n",
    "        result.append(train_metrics)\n",
    "        result.append(test_metrics)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for m in [\"ap\", \"f1\"]:\n",
    "\n",
    "    clf = AutoML(\n",
    "        task=\"classification\",\n",
    "        estimator_list=[\"xgboost\"],\n",
    "        early_stop=True,\n",
    "        eval_method=\"cv\",\n",
    "        metric=m,\n",
    "        time_budget=120,  # time budget in seconds\n",
    "        verbose=0,\n",
    "        skip_transform=True,  # don't preprocess data\n",
    "        auto_augment=False,  # don't augment rare classes\n",
    "        starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    "    )\n",
    "\n",
    "    out = cv(\n",
    "        clf=clf,\n",
    "        data=data,\n",
    "        features=features,\n",
    "        label_col=\"xtea_1kb_3end\",\n",
    "        rpm_filter=5,\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame(out)\n",
    "    out[\"metric\"] = m\n",
    "    result.append(out)\n",
    "\n",
    "result = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    result.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"metric\",\n",
    "    col=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does performance change with increasing numbers of donors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# initialize my custom model class\n",
    "print(\"Initializing Model object\")\n",
    "mdl = Model(data=data, features=features, label_col=\"xtea\", rpm_filter=5)\n",
    "\n",
    "# initialize splitter\n",
    "sgkf = StratifiedGroupKFold(n_splits=2)\n",
    "\n",
    "test_donor = mdl.data.donor_id.unique()[-1]\n",
    "train_donors = mdl.data.donor_id.unique()[:-1]\n",
    "\n",
    "result = []\n",
    "for n_donors in range(1, mdl.data.donor_id.nunique()):\n",
    "    print(f\"Running n_donors: {n_donors}\")\n",
    "    for i, (train_chrs, test_chrs) in enumerate(\n",
    "        sgkf.split(mdl.data, mdl.data[\"xtea\"], mdl.data[\"Chromosome\"])\n",
    "    ):\n",
    "        print(f\"Fold {i+1}\")\n",
    "        train_donor_idx = mdl.data.loc[\n",
    "            mdl.data[\"donor_id\"].isin(train_donors[0:n_donors]), :\n",
    "        ].index\n",
    "        test_donor_idx = mdl.data.loc[mdl.data[\"donor_id\"] == test_donor, :].index\n",
    "        assert (\n",
    "            len(np.intersect1d(train_donor_idx, test_donor_idx)) == 0\n",
    "        ), \"Donors in train and test set overlap\"\n",
    "        train_idx = np.intersect1d(train_donor_idx, train_chrs)\n",
    "        test_idx = np.intersect1d(test_donor_idx, test_chrs)\n",
    "        train_metrics, test_metrics, model_metrics = mdl.fit(\n",
    "            train_idx,\n",
    "            test_idx,\n",
    "            clf,\n",
    "            sample_col=\"cell_id\",\n",
    "            n_chr_splits=2,\n",
    "            n_sample_splits=2,\n",
    "        )\n",
    "        for metrics in [train_metrics, test_metrics]:\n",
    "            metrics[\"fold\"] = i + 1\n",
    "            metrics[\"n_donors\"] = n_donors\n",
    "            metrics.update(model_metrics)\n",
    "            result.append(metrics)\n",
    "result = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    result.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"n_donors\",\n",
    "    col=\"fold\",\n",
    "    row=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of feature importances\n",
    "fresult = (\n",
    "    result.explode([\"features\", \"feature_importances\"])\n",
    "    .groupby([\"n_donors\", \"features\"])\n",
    "    .agg({\"feature_importances\": \"mean\"})\n",
    "    .reset_index()\n",
    "    .pivot(index=\"features\", columns=\"n_donors\", values=\"feature_importances\")\n",
    ")\n",
    "sns.heatmap(fresult.astype(float)).set(xlabel=\"n_donors\", ylabel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does performance change with increasing numbers of chromosomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize my custom model class\n",
    "print(\"Initializing Model object\")\n",
    "mdl = Model(data=data, features=features, label_col=\"xtea\", rpm_filter=5)\n",
    "\n",
    "chr_sgkf = StratifiedGroupKFold(n_splits=5).split(\n",
    "    mdl.data, mdl.data[\"xtea\"], mdl.data[\"Chromosome\"]\n",
    ")\n",
    "cell_sgkf = StratifiedGroupKFold(n_splits=4)\n",
    "\n",
    "result = []\n",
    "test_chrs_idx = next(chr_sgkf)[1]\n",
    "train_chrs_idx = np.array([])\n",
    "for i, (_, chrs_idx) in enumerate(chr_sgkf):\n",
    "    print(f\"Running chr group {i+1}\")\n",
    "    train_chrs_idx = np.append(train_chrs_idx, chrs_idx)\n",
    "    print(f\"Training on {mdl.data.iloc[train_chrs_idx,:].Chromosome.unique()}\")\n",
    "    print(f\"Testing on {mdl.data.iloc[test_chrs_idx,:].Chromosome.unique()}\")\n",
    "    for j, (train_cell_idx, test_cell_idx) in enumerate(\n",
    "        cell_sgkf.split(mdl.data, mdl.data[\"xtea\"], mdl.data[\"cell_id\"])\n",
    "    ):\n",
    "        print(f\"Fold {j+1}\")\n",
    "        train_idx = np.intersect1d(train_cell_idx, train_chrs_idx)\n",
    "        test_idx = np.intersect1d(test_cell_idx, test_chrs_idx)\n",
    "        train_metrics, test_metrics, model_metrics = mdl.fit(\n",
    "            train_idx,\n",
    "            test_idx,\n",
    "            clf,\n",
    "            sample_col=\"cell_id\",\n",
    "            n_chr_splits=2,\n",
    "            n_sample_splits=2,\n",
    "        )\n",
    "        for metrics in [train_metrics, test_metrics]:\n",
    "            metrics[\"fold\"] = j + 1\n",
    "            metrics[\"chr_groups\"] = i + 1\n",
    "            metrics.update(model_metrics)\n",
    "            result.append(metrics)\n",
    "\n",
    "result = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    result.explode([\"precision\", \"adjusted_locus_recall\"]),\n",
    "    x=\"adjusted_locus_recall\",\n",
    "    y=\"precision\",\n",
    "    hue=\"chr_groups\",\n",
    "    col=\"fold\",\n",
    "    row=\"stage\",\n",
    "    kind=\"line\",\n",
    ").set(xlim=(0, 1), ylim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of feature importances\n",
    "fresult = (\n",
    "    result.explode([\"features\", \"feature_importances\"])\n",
    "    .groupby([\"chr_groups\", \"features\"])\n",
    "    .agg({\"feature_importances\": \"mean\"})\n",
    "    .reset_index()\n",
    "    .pivot(index=\"features\", columns=\"chr_groups\", values=\"feature_importances\")\n",
    ")\n",
    "sns.heatmap(fresult.astype(float)).set(xlabel=\"chr_groups\", ylabel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
