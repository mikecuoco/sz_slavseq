{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the SLAV-calling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PosixPath\n",
    "from tempfile import NamedTemporaryFile\n",
    "import warnings\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")\n",
    "\n",
    "# ML\n",
    "from sklearn import metrics, model_selection, ensemble\n",
    "from flaml import AutoML\n",
    "from flaml.automl.data import get_output_from_log\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "# custom\n",
    "from scripts.get_labels import label_windows\n",
    "from scripts.fit import SampleChrSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for model testing\n",
    "\n",
    "Evaluation Strategy: Train on 1/2 donors and 1/2 chromosomes, test on the other half of donors and chromosomes.\n",
    "\n",
    "Tuning Spliting Strategy:\n",
    "\t\n",
    "1. Tune within each training set? Splitting again on chromosomes and donors\n",
    "2. Tune on CommonBrain donor, splitting on chromosomes and cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auprc(y_true, y_pred, pos_label: int, sample_weight=None):\n",
    "    \"Area under the precision-recall curve.\"\n",
    "\n",
    "    prc = metrics.precision_recall_curve(\n",
    "        y_true, y_pred, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    auprc = metrics.auc(prc[1], prc[0])\n",
    "\n",
    "    return auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auprc_flaml(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    estimator,\n",
    "    labels,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    weight_val=None,\n",
    "    weight_train=None,\n",
    "    config=None,\n",
    "    groups_val=None,\n",
    "    groups_train=None,\n",
    "):\n",
    "    \"Custom auprc metric for FLAML\"\n",
    "    # TODO: not working, y should be 1d array but is 2d\n",
    "\n",
    "    y_pred = estimator.predict_proba(X_val)\n",
    "    val_auprc = auprc(y_val, y_pred, pos_label=1, sample_weight=weight_val)\n",
    "    y_pred = estimator.predict_proba(X_train)\n",
    "    train_auprc = auprc(y_train, y_pred, pos_label=1, sample_weight=weight_train)\n",
    "    return 1 - val_auprc, {\"train_auprc\": 1 - train_auprc, \"val_auprc\": 1 - val_auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename: PosixPath):\n",
    "    \"\"\"\n",
    "    Reads a parquet file, processes labels, shrinks data types, and checks for inf/nan values, returning a pandas dataframe.\n",
    "    :param filename: path to parquet file\n",
    "    \"\"\"\n",
    "\n",
    "    # read with pyarrow\n",
    "    assert filename.suffix == \".parquet\" or \".pqt\", \"filename must be a parquet file\"\n",
    "    df = pq.read_table(filename).to_pandas()\n",
    "\n",
    "    # fix labels\n",
    "    df[\"label\"] = df[\"label\"].replace({\"xtea_1kb_3end\": \"knrgl\"})\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: \"unknown\" if x != \"knrgl\" else x)\n",
    "    df[\"label_encoded\"] = df[\"label\"].map({\"knrgl\": 1, \"unknown\": 0})\n",
    "\n",
    "    # cleanup columns\n",
    "    df = df.drop(columns=[c for c in df.columns if df[c].dtype == bool])\n",
    "    df = df.loc[df[\"rpm\"] >= 2]\n",
    "    for c in df.columns:\n",
    "        if c == \"rpm\":\n",
    "            min_val = np.finfo(np.float32).min\n",
    "            max_val = np.finfo(np.float32).max\n",
    "            df[c] = np.clip(df[c], min_val, max_val).astype(np.float32)\n",
    "        elif df[c].dtype == np.float64:\n",
    "            df[c] = df[c].astype(np.float16)\n",
    "        elif df[c].dtype == np.int64:\n",
    "            df[c] = df[c].astype(np.int32)  # must use int32 for chromosomal positions\n",
    "        if (df[c].dtype == np.float16) or (df[c].dtype == np.int32):\n",
    "            assert not np.isinf(df[c]).any(), f\"{c} column contains inf values\"\n",
    "            assert not df[c].isna().any(), f\"{c} column contains nan values\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~40 min, 120 GB RAM\n",
    "DATADIR = Path(\"../results/model/get_labels/\")\n",
    "data = pd.concat([read_data(f) for f in DATADIR.rglob(\"*pqt\")])\n",
    "\n",
    "# remove low quality cells\n",
    "with open(\"../resources/bad_cells.txt\", \"r\") as f:\n",
    "    bad_cells = [line.strip() for line in f.readlines()]\n",
    "data = data[~data[\"cell_id\"].isin(bad_cells)]\n",
    "\n",
    "# keep autosomes\n",
    "data = data.loc[data[\"Chromosome\"].isin([f\"chr{i}\" for i in range(1, 23)])]\n",
    "\n",
    "# keep windows with at least 2 reads-per-millions\n",
    "data = data.loc[data[\"rpm\"] >= 2]\n",
    "\n",
    "# remove blacklist region\n",
    "mhc = pd.read_csv(\n",
    "    \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.0/GRCh38/OtherDifficult/GRCh38_MHC.bed.gz\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "kir = pd.read_csv(\n",
    "    \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.0/GRCh38/OtherDifficult/GRCh38_KIR.bed.gz\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "trs = pd.read_csv(\n",
    "    \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.0/GRCh38/LowComplexity/GRCh38_AllTandemRepeats_201to10000bp_slop5.bed.gz\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "segdups = pd.read_csv(\n",
    "    \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.0/GRCh38/SegmentalDuplications/GRCh38_segdups.bed.gz\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "gaps = pd.read_csv(\n",
    "    \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.0/GRCh38/OtherDifficult/GRCh38_gaps_slop15kb.bed.gz\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "false_dup = pd.read_csv(\n",
    "    \"https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/genome-stratifications/v3.0/GRCh38/OtherDifficult/GRCh38_false_duplications_correct_copy.bed.gz\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "blacklist = pd.concat([mhc, trs, segdups, gaps, false_dup, kir])\n",
    "\n",
    "data = label_windows(data, blacklist, \"blacklist\")\n",
    "data = data.loc[data[\"blacklist\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of reads per million and number of reads for each donor by label\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for donor_id, df in data.groupby(\"donor_id\"):\n",
    "    sns.ecdfplot(\n",
    "        df,\n",
    "        x=\"rpm\",\n",
    "        hue=\"label\",\n",
    "        ax=ax1,\n",
    "        label=donor_id,\n",
    "        alpha=0.5,\n",
    "        complementary=True,\n",
    "        stat=\"count\",\n",
    "        log_scale=(True, True),\n",
    "    )\n",
    "    sns.ecdfplot(\n",
    "        df,\n",
    "        x=\"n_reads\",\n",
    "        hue=\"label\",\n",
    "        ax=ax2,\n",
    "        label=donor_id,\n",
    "        alpha=0.5,\n",
    "        complementary=True,\n",
    "        stat=\"count\",\n",
    "        log_scale=(True, True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of reads per million and number of reads for each donor by label\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for cell_id, df in data[data[\"donor_id\"] == \"CommonBrain\"].groupby(\"cell_id\"):\n",
    "    sns.ecdfplot(\n",
    "        df,\n",
    "        x=\"rpm\",\n",
    "        hue=\"label\",\n",
    "        ax=ax1,\n",
    "        alpha=0.5,\n",
    "        complementary=True,\n",
    "        stat=\"count\",\n",
    "        log_scale=(True, True),\n",
    "    )\n",
    "    sns.ecdfplot(\n",
    "        df,\n",
    "        x=\"n_reads\",\n",
    "        hue=\"label\",\n",
    "        ax=ax2,\n",
    "        alpha=0.5,\n",
    "        complementary=True,\n",
    "        stat=\"count\",\n",
    "        log_scale=(True, True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features\n",
    "features = []\n",
    "keys = [\"_mean\", \"frac\", \"gini\", \"bias\"]\n",
    "for c in data.columns:\n",
    "    for k in keys:\n",
    "        if k in c:\n",
    "            features.append(c)\n",
    "print(\"Features:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    features: list,\n",
    "    time_budget: int,\n",
    "    axes=None,\n",
    "    label: str = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # check that features are in data\n",
    "    for f in features + [\"label_encoded\", \"donor_id\"]:\n",
    "        assert f in data.columns, f\"{f} not in data.columns\"\n",
    "\n",
    "    # split data into train and tune for hyperparameter tuning\n",
    "    sgkf = SampleChrSplitter(X=train, y=train[\"label_encoded\"], sample_col=\"donor_id\")\n",
    "\n",
    "    with NamedTemporaryFile() as logfile:\n",
    "        clf = AutoML()\n",
    "        print(\"fitting model...\")\n",
    "        clf.fit(\n",
    "            task=\"classification\",\n",
    "            X_train=train[features],\n",
    "            y_train=train[\"label_encoded\"],\n",
    "            n_jobs=16,\n",
    "            estimator_list=[\"xgboost\", \"rf\", \"xgb_limitdepth\"],\n",
    "            early_stop=True,\n",
    "            eval_method=\"cv\",\n",
    "            split_type=sgkf,\n",
    "            log_file_name=logfile.name,\n",
    "            time_budget=time_budget,  # time budget in seconds\n",
    "            verbose=0,\n",
    "            **kwargs,\n",
    "        )\n",
    "        print(\"done\")\n",
    "        # get learning curve\n",
    "        (\n",
    "            time_history,\n",
    "            best_valid_loss_history,\n",
    "            valid_loss_history,\n",
    "            config_history,\n",
    "            metric_history,\n",
    "        ) = get_output_from_log(filename=logfile.name, time_budget=time_budget)\n",
    "\n",
    "    print(\"best estimator:\", clf.best_estimator)\n",
    "    print(\"best config:\", clf.best_config)\n",
    "    print(\"best loss:\", clf.best_loss)\n",
    "\n",
    "    # make subplots\n",
    "    print(\"plotting...\")\n",
    "    if not axes:\n",
    "        _, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "        assert axes.shape == (2, 2), \"axes must be 2x2\"\n",
    "\n",
    "    # class balance\n",
    "    train_counts = train.value_counts(\"label\").reset_index(name=\"count\")\n",
    "    test_counts = test.value_counts(\"label\").reset_index(name=\"count\")\n",
    "    plot_df = pd.concat(\n",
    "        [train_counts.assign(stage=\"train\"), test_counts.assign(stage=\"test\")]\n",
    "    )\n",
    "    plot_df[\"percent\"] = (\n",
    "        plot_df[\"count\"] / plot_df.groupby(\"stage\")[\"count\"].transform(\"sum\")\n",
    "    ) * 100\n",
    "    sns.barplot(\n",
    "        x=\"stage\",\n",
    "        y=\"percent\",\n",
    "        data=plot_df,\n",
    "        order=[\"train\", \"test\"],\n",
    "        hue=\"label\",\n",
    "        ax=axes[0, 0],\n",
    "    )\n",
    "    axes[0, 0].title.set_text(\"Class Balance\")\n",
    "    axes[0, 0].set_xlabel(\"Label\")\n",
    "    axes[0, 0].set_ylabel(\"Percent\")\n",
    "    axes[0, 0].set_yscale(\"log\")\n",
    "    # add counts to bars\n",
    "    for p in axes[0, 0].patches:\n",
    "        axes[0, 0].annotate(\n",
    "            f\"{p.get_height():.2f}%\",\n",
    "            (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            xytext=(0, 10),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "    axes[0, 0].yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "    # learning curve\n",
    "    axes[0, 1].step(time_history, 1 - np.array(best_valid_loss_history), where=\"post\")\n",
    "    axes[0, 1].title.set_text(\"Learning Curve\")\n",
    "    axes[0, 1].set_xlabel(\"Wall Clock Time (s)\")\n",
    "    axes[0, 1].set_ylabel(\"Test Average Precision\")\n",
    "\n",
    "    # metrics\n",
    "    metrics.PrecisionRecallDisplay.from_estimator(\n",
    "        clf, test[features], test[\"label_encoded\"], ax=axes[1, 0]\n",
    "    )\n",
    "    metrics.RocCurveDisplay.from_estimator(\n",
    "        clf, test[features], test[\"label_encoded\"], ax=axes[1, 1]\n",
    "    )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "Path(\"../results/20210731_model_testing\").mkdir(exist_ok=True)\n",
    "for rpm in [10, 50, 100, 500, 1000]:\n",
    "    my_data = data.loc[data[\"rpm\"] >= rpm, :]\n",
    "    sgkf = SampleChrSplitter(\n",
    "        X=my_data, y=my_data[\"label_encoded\"], sample_col=\"donor_id\"\n",
    "    ).split(my_data)\n",
    "    train_idx, test_idx = next(sgkf)\n",
    "    clf = fit(\n",
    "        train=my_data.iloc[train_idx, :].reset_index(drop=True),\n",
    "        test=my_data.iloc[test_idx, :].reset_index(drop=True),\n",
    "        features=features,\n",
    "        time_budget=600,\n",
    "        metric=\"ap\",\n",
    "        skip_transform=True,  # don't preprocess data\n",
    "        auto_augment=False,  # don't augment rare classes\n",
    "        starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    "    )\n",
    "\n",
    "    # append results to list\n",
    "    y_test = my_data.iloc[test_idx, :][\"label_encoded\"]\n",
    "    y_pred = clf.predict_proba(my_data.iloc[test_idx, :][features])\n",
    "    res.append(\n",
    "        {\"rpm\": rpm, \"ap\": metrics.average_precision_score(y_test, y_pred[:, 1])}\n",
    "    )\n",
    "    plt.savefig(f\"../results/20210731_model_testing/rpm_{rpm}.png\")\n",
    "\n",
    "sns.lineplot(data=pd.DataFrame(res), x=\"rpm\", y=\"ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data[\"rpm\"] > 20]\n",
    "sgkf = SampleChrSplitter(X=data, y=data[\"label_encoded\"], sample_col=\"donor_id\").split(\n",
    "    data\n",
    ")\n",
    "train_idx, test_idx = next(sgkf)\n",
    "clf = fit(\n",
    "    train=data.iloc[train_idx, :].reset_index(drop=True),\n",
    "    test=data.iloc[test_idx, :].reset_index(drop=True),\n",
    "    features=features,\n",
    "    time_budget=100,\n",
    "    metric=\"ap\",\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune and evaluate XGBoost model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_CV(data, features, time_budget=600, **kwargs):\n",
    "\t\"\"\"\n",
    "\tPerform 4-fold nested cross-validation to tune hyperparameters and evaluate model performance.\n",
    "\t:param data: pd.DataFrame with features and labels\n",
    "\t:param features: list of feature names, corresponding to columns in data\n",
    "\t:param time_budget: time budget in seconds for tuning per fold\n",
    "\t\"\"\"\n",
    "\n",
    "\t# initialize subplots\n",
    "\t_, axs = plt.subplot_mosaic([[\"TrClassBal\",\"TrPRC\",\"TrROC\",\"LC\"],[\"VaClassBal\",\"VaPRC\",\"VaROC\",\"LC\"]], figsize=(17, 8))\n",
    "\tplt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\t\n",
    "\n",
    "\t# split data into train and test for evaluation\n",
    "\teval_sgkf = SampleChrSplitter(X=data, y=data[\"label_encoded\"], sample_col=\"donor_id\")\n",
    "\n",
    "\tfor i, (train_idx, test_idx) in enumerate(eval_sgkf.split(data)):\n",
    "\t\tprint(f\"Fold {i+1}\")\n",
    "\t\ttrain = data.iloc[train_idx, :].reset_index(drop=True)\n",
    "\t\tvalid = data.iloc[test_idx, :].reset_index(drop=True)\n",
    "\t\t\t\n",
    "\t\tprint(f\"Training on {train.shape[0]} windows\")\n",
    "\t\tprint(\"Training on donors:\", train[\"donor_id\"].unique())\n",
    "\t\tprint(\"Training on chromosomes:\", train[\"Chromosome\"].unique())\n",
    "\n",
    "\t\tclf = fit(train, features, time_budget=time_budget, **kwargs)\n",
    "\n",
    "\t\t# PLOTS\n",
    "\t\tlabel = f\"Fold {i+1}: {clf.best_estimator}\"\n",
    "\n",
    "\t\tmetrics.PrecisionRecallDisplay.from_estimator(clf, train[features], train[\"label_encoded\"], ax=axs[\"TrPRC\"], name=label)\n",
    "\t\tmetrics.RocCurveDisplay.from_estimator(clf, train[features], train[\"label_encoded\"], ax=axs[\"TrROC\"], name=label)\n",
    "\t\tmetrics.PrecisionRecallDisplay.from_estimator(clf, valid[features], valid[\"label_encoded\"], ax=axs[\"VaPRC\"], name=label)\n",
    "\t\tmetrics.RocCurveDisplay.from_estimator(clf, valid[features], valid[\"label_encoded\"], ax=axs[\"VaROC\"], name=label)\n",
    "\n",
    "\t\t# learning curve\n",
    "\t\taxs[\"LC\"].step(clf.time_history, 1- np.array(clf/.best_valid_loss_history), label=label)\n",
    "\n",
    "\t\t# class balance\n",
    "\t\ttrain_bal = train.value_counts(\"label\").reset_index().rename(columns={0:\"count\"})\n",
    "\t\taxs[\"TrClassBal\"].bar(i, train_bal[\"count\"][0], label=train_bal[\"label\"][0])\n",
    "\t\taxs[\"TrClassBal\"].bar(i+1, train_bal[\"count\"][1], label=train_bal[\"label\"][1])\n",
    "\t\tvalid_bal = valid.value_counts(\"label\").reset_index().rename(columns={0:\"count\"})\n",
    "\t\taxs[\"VaClassBal\"].bar(i, valid_bal[\"count\"], label=valid_bal[\"label\"][0])\n",
    "\t\taxs[\"VaClassBal\"].bar(i+1, valid_bal[\"count\"], label=valid_bal[\"label\"][1])\n",
    "\t\n",
    "\t\n",
    "\taxs[\"TrClassBal\"].title.set_text(\"Training Class Balance\")\n",
    "\taxs[\"TrClassBal\"].legend()\n",
    "\taxs[\"TrClassBal\"].set_yscale(\"log\")\n",
    "\n",
    "\taxs[\"VaClassBal\"].title.set_text(\"Validation Class Balance\")\n",
    "\taxs[\"VaClassBal\"].legend()\n",
    "\taxs[\"VaClassBal\"].set_yscale(\"log\")\n",
    "\n",
    "\taxs[\"TrPRC\"].title.set_text(\"Training Precision-Recall\")\n",
    "\taxs[\"TrPRC\"].set_xlabel(\"Recall\")\n",
    "\taxs[\"TrPRC\"].set_ylabel(\"Precision\")\n",
    "\n",
    "\taxs[\"TrROC\"].title.set_text(\"Training ROC\")\n",
    "\taxs[\"TrROC\"].set_xlabel(\"False Positive Rate\")\n",
    "\taxs[\"TrROC\"].set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "\taxs[\"VaPRC\"].title.set_text(\"Validation Precision-Recall\")\n",
    "\taxs[\"VaPRC\"].set_xlabel(\"Recall\")\n",
    "\taxs[\"VaPRC\"].set_ylabel(\"Precision\")\n",
    "\n",
    "\taxs[\"VaROC\"].title.set_text(\"Validation ROC\")\n",
    "\taxs[\"VaROC\"].set_xlabel(\"False Positive Rate\")\n",
    "\taxs[\"VaROC\"].set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "\taxs[\"LC\"].title.set_text(\"Learning curve\")\n",
    "\taxs[\"LC\"].set_xlabel(\"Wall Clock Time (s)\")\n",
    "\taxs[\"LC\"].set_ylabel(\"Test Average Precision\")\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_CV(\n",
    "    data[data[\"rpm\"] > 5],\n",
    "    features + [\"rpm\"],\n",
    "    time_budget=600,\n",
    "    metric=\"ap\",\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_CV(\n",
    "    data[data[\"rpm\"] > 10],\n",
    "    features,\n",
    "    time_budget=600,\n",
    "    metric=\"ap\",\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_CV(\n",
    "    data[data[\"rpm\"] > 10],\n",
    "    features + [\"rpm\"],\n",
    "    time_budget=600,\n",
    "    metric=\"ap\",\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "Evaluate performance as a function of:\n",
    "1. donors included\n",
    "2. windows/donor\n",
    "3. Chromosomes\n",
    "4. n_reads (or rpm) \n",
    "\n",
    "\n",
    "With/without\n",
    "1. rpm as a feature\n",
    "2. skip_transform=False\n",
    "3. auto_augment=True\n",
    "4. starting_points=\"dynamic\"\n",
    "5. EN score as a feature\n",
    "6. quantiles vs mean\n",
    "\n",
    "Compute FP/FN rate!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
