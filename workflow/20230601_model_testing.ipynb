{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SLAV-calling model on 5 individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "\n",
    "print(f\"Number of CPUs in this system: {os.cpu_count()}\")\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "import pyranges as pr\n",
    "\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "from flaml import AutoML\n",
    "\n",
    "# import ray\n",
    "\n",
    "# print(f\"ray: {ray.__version__}\")\n",
    "# print(\"initializing ray...\")\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "# ray.init()\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# custom\n",
    "from scripts.get_labels import label_windows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RepeatMasker, KNRGL, and blacklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsk = pd.read_csv(\n",
    "    \"/iblm/netapp/data4/mcuoco/sz_slavseq/resources/rmsk_1kb_3end.bed\",\n",
    "    sep=\"\\t\",\n",
    "    usecols=[0, 1, 2],\n",
    "    names=[\"Chromosome\", \"Start\", \"End\"],\n",
    ")\n",
    "\n",
    "GRC_blacklist = pd.read_csv(\n",
    "    \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_GRC_exclusions.bed\",\n",
    "    sep=\"\\t\",\n",
    "    skiprows=3,\n",
    ")\n",
    "GRC_blacklist.rename(\n",
    "    columns={\"#sequence\": \"Chromosome\", \"sequenceStart\": \"Start\", \"sequenceEnd\": \"End\"},\n",
    "    inplace=True,\n",
    ")\n",
    "segdups = pr.read_bed(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/cf.10xgenomics.com/supp/genome/GRCh38/segdups.bedpe\"\n",
    ").df\n",
    "sv_blacklist = pr.read_bed(\n",
    "    \"/iblm/logglun02/mcuoco/workflows/sz_slavseq/cf.10xgenomics.com/supp/genome/GRCh38/sv_blacklist.bed\"\n",
    ").df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chromosomes and lengths\n",
    "chr_lengths = {}\n",
    "with open(\"/iblm/netapp/data4/mcuoco/sz_slavseq/resources/hs38d1.fa.fai\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        chr_lengths[l[0]] = int(l[1])\n",
    "\n",
    "# remove chromosomes not in rmsk\n",
    "chr_rm = []\n",
    "for c in chr_lengths.keys():\n",
    "    if c not in rmsk.Chromosome.unique():\n",
    "        chr_rm.append(c)\n",
    "\n",
    "for c in chr_rm:\n",
    "    del chr_lengths[c]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and label bam windows (takes ~3 hrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get KNRGL and window files for each donor\n",
    "# donors = [\"CommonBrain\",1,3,4,8,27]\n",
    "donors = [\"CommonBrain\"]\n",
    "donor_files = {}\n",
    "for d in donors:\n",
    "    donor_files[d] = defaultdict(list)\n",
    "    for f in Path(\"/iblm/netapp/data4/mcuoco/sz_slavseq/resources\").rglob(\n",
    "        f\"{d}_insertions_1kb_3nd.bed\"\n",
    "    ):\n",
    "        donor_files[d][\"KNRGL\"] = str(f)\n",
    "    for f in Path(\n",
    "        f\"/iblm/netapp/data4/mcuoco/sz_slavseq/results/model/get_features/{d}\"\n",
    "    ).rglob(\"*.pqt\"):\n",
    "        donor_files[d][\"features\"].append(str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to read in the data\n",
    "# filter out windows with nr1 < 4 and decoy chromosomes\n",
    "def read_data(filename: str):\n",
    "    return pq.read_table(\n",
    "        filename,\n",
    "        filters=[\n",
    "            (\"nr1\", \">=\", 1),\n",
    "            (\"Chromosome\", \"not in\", chr_rm),\n",
    "        ],\n",
    "    ).to_pandas()\n",
    "\n",
    "\n",
    "def collate_labels(x):\n",
    "    if x.RMSK:\n",
    "        return \"RMSK\"\n",
    "    elif x.RMSK_10kb:\n",
    "        return \"RMSK_10kb\"\n",
    "    elif x.KNRGL:\n",
    "        return \"KNRGL\"\n",
    "    elif x.KNRGL_10kb:\n",
    "        return \"KNRGL_10kb\"\n",
    "    elif x.GRC_blacklist or x.SV_blacklist or x.segdups:\n",
    "        return \"blacklist\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "\n",
    "# iterate over donors and read in the data\n",
    "# label the windows\n",
    "df = []\n",
    "for d in donors:\n",
    "    print(f\"Reading {len(donor_files[d]['features'])} files for donor {d} ..\")\n",
    "    ddf = pd.concat([read_data(f) for f in tqdm(donor_files[d][\"features\"])])\n",
    "    knrgl = pd.read_csv(\n",
    "        donor_files[d][\"KNRGL\"],\n",
    "        sep=\"\\t\",\n",
    "        names=[\"Chromosome\", \"Start\", \"End\"],\n",
    "        usecols=[0, 1, 2],\n",
    "    )\n",
    "    ddf = label_windows(ddf, knrgl, \"KNRGL\")\n",
    "    # ddf = label_windows(ddf, knrgl, \"KNRGL_10kb\")\n",
    "    ddf = label_windows(ddf, rmsk, \"RMSK\")\n",
    "    # ddf = label_windows(ddf, rmsk_10kb, \"RMSK_10kb\")\n",
    "    ddf = label_windows(ddf, GRC_blacklist, \"GRC_blacklist\")\n",
    "    ddf = label_windows(ddf, segdups, \"segdups\")\n",
    "    ddf = label_windows(ddf, sv_blacklist, \"SV_blacklist\")\n",
    "    ddf[\"label\"] = ddf.apply(collate_labels, axis=1)\n",
    "\n",
    "    df.append(ddf)\n",
    "\n",
    "\n",
    "df = pd.concat(df)\n",
    "\n",
    "# remove low quality cells\n",
    "bad_cells = [\n",
    "    \"USD3_F3_S151\",\n",
    "    \"USD3_C3_S148\",\n",
    "    \"USD3_E3_S150\",\n",
    "    \"USD3_C5_S163\",\n",
    "    \"USD3_G3_S152\",\n",
    "    \"USD4E2_S141\",\n",
    "    \"plate2_E7_S101\",\n",
    "    \"ush8_D7_S51\",\n",
    "    \"ush27_A7_S159\",\n",
    "]\n",
    "df = df.loc[~df[\"cell_id\"].isin(bad_cells), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove windows with any of the above labels\n",
    "# df = df.loc[~df[[\"GRC_blacklist\", \"segdups\", \"sv_blacklist\", \"rmsk\"]].any(axis=1), :]\n",
    "# df.drop([\"GRC_blacklist\", \"segdups\", \"sv_blacklist\", \"rmsk\"], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read labelled windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df = pq.read_table(\"5donors_labelled.pqt\").to_pandas()\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check distribution of classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- plot absolute window numbers as a function of read 1 threshold colored by class\n",
    "- plot ratio\n",
    "- include false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subplots\n",
    "fig, axes = plt.subplots(len(donors), 3, figsize=(13, 4 * len(donors)))\n",
    "fig.tight_layout(w_pad=5.0, h_pad=4.0)\n",
    "hue_order = df[\"label\"].unique()\n",
    "\n",
    "for i, d in enumerate(df[\"donor_id\"].unique()):\n",
    "    ddf = df.loc[(df[\"donor_id\"] == d), :]\n",
    "    print(f\"Number of windows for {d}: {ddf.shape[0]}\")\n",
    "    title = f\"donor {d}\"\n",
    "    sns.ecdfplot(\n",
    "        data=ddf, x=\"nr1\", hue=\"label\", hue_order=hue_order, ax=axes[i, 0], legend=False\n",
    "    ).set(xscale=\"log\", title=title, xlabel=\"# Read 1\")\n",
    "    sns.ecdfplot(\n",
    "        data=ddf,\n",
    "        x=\"nr1\",\n",
    "        hue=\"label\",\n",
    "        hue_order=hue_order,\n",
    "        ax=axes[i, 1],\n",
    "        legend=False,\n",
    "        stat=\"count\",\n",
    "        complementary=True,\n",
    "    ).set(xscale=\"log\", yscale=\"log\", title=title, xlabel=\"# Read 1\")\n",
    "    # sns.histplot(data=ddf, x=\"nr1\", hue=\"label\", hue_order=hue_order, ax=axes[i,1], log_scale=True, fill=False, bins=100, element=\"step\", legend=False).set(yscale=\"log\", title=title, xlabel=\"# Read 1\")\n",
    "\n",
    "    plot_df = (\n",
    "        ddf.groupby([\"label\", \"cell_id\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename({0: \"count\"}, axis=1)\n",
    "    )\n",
    "\n",
    "    sns.stripplot(\n",
    "        data=plot_df,\n",
    "        y=\"label\",\n",
    "        x=\"count\",\n",
    "        alpha=0.5,\n",
    "        hue=\"label\",\n",
    "        hue_order=hue_order,\n",
    "        ax=axes[i, 2],\n",
    "        legend=False,\n",
    "    ).set(xscale=\"log\", title=title, xlabel=\"# Windows / cell\", ylabel=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model's hyperparameters using [Microsoft's FLAML library](https://microsoft.github.io/FLAML/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "keys = [\"ML\", \"MA\", \"MS\", \"ME\", \"AS\", \"frac\", \"bias\"]\n",
    "for c in df.columns:\n",
    "    for k in keys:\n",
    "        if (k in c) and (\"r2\" not in c) and (\"prop\" not in c):\n",
    "            features.append(c)\n",
    "\n",
    "# encode labels\n",
    "df[\"label_encoded\"] = df[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set flaml settings\n",
    "# NOTE: Don't try logistic regression, it's too slow, doesn't converge, and doesn't perform well\n",
    "flaml_settings = dict(\n",
    "    task=\"classification\",\n",
    "    n_jobs=16,\n",
    "    estimator_list=[\"xgboost\", \"rf\"],\n",
    "    early_stop=True,\n",
    "    skip_transform=True,  # don't preprocess data\n",
    "    auto_augment=False,  # don't augment rare classes\n",
    "    starting_points=\"static\",  # use data-independent hyperparameterstarting points\n",
    "    log_training_metric=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data to tune on\n",
    "eval_chr = \"chr1\"\n",
    "eval_data = df.loc[(df[\"Chromosome\"] == eval_chr), :].reset_index()\n",
    "# eval_data = df.loc[(df[\"donor_id\"] != \"CommonBrain\") & (df[\"Chromosome\"] == eval_chr) & (df[\"label\"] != \"RMSK\"), :].reset_index()\n",
    "\n",
    "# define data for final evaluation\n",
    "tune_data = df.loc[\n",
    "    (df[\"donor_id\"] == \"CommonBrain\") & (df[\"Chromosome\"] != eval_chr), :\n",
    "].reset_index()\n",
    "# tune_data = df.loc[(df[\"donor_id\"] == \"CommonBrain\") & (df[\"Chromosome\"] != eval_chr) & (df[\"label\"] != \"RMSK\"), :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation function\n",
    "def precision_recall(pred: pd.DataFrame, insertions: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall for a binary classifier\n",
    "    pred: predicted labels for genomic windows\n",
    "    insertions: L1 annotations\n",
    "    \"\"\"\n",
    "    assert \"pred\" in pred.columns, \"pred must have column 'pred'\"\n",
    "    assert set(pred.pred.unique()) == set([0, 1]), \"pred must be binary\"\n",
    "\n",
    "    for col in [\"Chromosome\", \"Start\", \"End\"]:\n",
    "        assert col in pred.columns, f\"pred must have column {col}\"\n",
    "        assert col in rmsk.columns, f\"rmsk must have column {col}\"\n",
    "        assert col in knrgl.columns, f\"knrgl must have column {col}\"\n",
    "\n",
    "    # only consider insertions that have windows\n",
    "    insertions = pr.PyRanges(insertions).overlap(pr.PyRanges(pred)).df\n",
    "\n",
    "    # how many insertions were detected?\n",
    "    y_pos = pred.loc[pred[\"pred\"] == 1, :]\n",
    "    tp = len(pr.PyRanges(insertions).overlap(pr.PyRanges(y_pos)).df)\n",
    "\n",
    "    # how many insertions were false positives?\n",
    "    fp = len(pr.PyRanges(insertions).overlap(pr.PyRanges(y_pos), invert=True).df)\n",
    "\n",
    "    # how many insertions were missed?\n",
    "    y_neg = pred.loc[pred[\"pred\"] == 0, :]\n",
    "    fn = len(pr.PyRanges(insertions).overlap(pr.PyRanges(y_neg)).df)\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom splitter for AutoML split_type argument\n",
    "# \"A valid splitter object is an instance of a derived class of scikit-learn KFold and have split and get_n_splits methods with the same signatures. Set eval_method to \"cv\" to use the splitter object.\"\n",
    "class SampleChromosomeSplitter:\n",
    "    def __init__(self, n_splits, X, y, sample_col):\n",
    "        \"\"\"\n",
    "        Initialize the splitter object\n",
    "        X: pandas dataframe with columns \"donor_id\" and \"Chromosome\"\n",
    "        y: pandas series with labels\n",
    "        \"\"\"\n",
    "        assert sample_col in X.columns, f\"X must have column {sample_col}\"\n",
    "        assert \"Chromosome\" in X.columns, \"X must have column 'Chromosome'\"\n",
    "        self.n_splits = n_splits\n",
    "        self.sample_array = X[sample_col].values\n",
    "        self.chr_array = X[\"Chromosome\"].values\n",
    "        self.y = y\n",
    "\n",
    "    def split(self, X):\n",
    "        assert (\n",
    "            X.shape[0] == self.y.shape[0]\n",
    "        ), \"X and y must have the same number of rows\"\n",
    "        for chr_train_index, chr_test_index in StratifiedGroupKFold(\n",
    "            n_splits=self.n_splits\n",
    "        ).split(X, self.y, groups=self.chr_array):\n",
    "            for sample_train_index, sample_test_index in StratifiedGroupKFold(\n",
    "                n_splits=self.n_splits\n",
    "            ).split(X, self.y, groups=self.sample_array):\n",
    "                train_index = np.intersect1d(sample_train_index, chr_train_index)\n",
    "                test_index = np.intersect1d(sample_test_index, chr_test_index)\n",
    "\n",
    "                yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits ^ 2\n",
    "\n",
    "\n",
    "# test splitter\n",
    "sample_col = \"cell_id\"\n",
    "splitter = SampleChromosomeSplitter(\n",
    "    n_splits=5, X=tune_data, y=tune_data[\"label_encoded\"], sample_col=sample_col\n",
    ")\n",
    "for train_index, test_index in splitter.split(tune_data[features]):\n",
    "    train = tune_data.iloc[train_index]\n",
    "    test = tune_data.iloc[test_index]\n",
    "\n",
    "    print(f\"Train: {len(train_index)}, Test: {len(test_index)}\")\n",
    "    print(\n",
    "        f\"Training on samples {train[sample_col].nunique()} and Chromosomes {','.join(train['Chromosome'].unique())}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Testing on samples {test[sample_col].nunique()} and Chromosomes {','.join(test['Chromosome'].unique())}\"\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit using CV\n",
    "# TODO: use holdout data for testing\n",
    "clf = AutoML()\n",
    "\n",
    "clf.fit(\n",
    "    X_train=tune_data[features],\n",
    "    y_train=tune_data[\"label_encoded\"],\n",
    "    metric=\"f1\",\n",
    "    eval_method=\"cv\",\n",
    "    split_type=SampleChromosomeSplitter(\n",
    "        n_splits=5, X=tune_data, y=tune_data[\"label_encoded\"], sample_col=\"cell_id\"\n",
    "    ),\n",
    "    log_file_name=\"flaml_cv.log\",\n",
    "    time_budget=600,\n",
    "    **flaml_settings,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use single train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test chromosomes\n",
    "chrom_sgkf = StratifiedGroupKFold(n_splits=5).split(\n",
    "    tune_data, tune_data[\"label_encoded\"], groups=tune_data[\"Chromosome\"]\n",
    ")\n",
    "chr_train_index, chr_test_index = next(chrom_sgkf)\n",
    "train_chrs = tune_data.iloc[chr_train_index, :][\"Chromosome\"].unique()\n",
    "test_chrs = tune_data.iloc[chr_test_index, :][\"Chromosome\"].unique()\n",
    "assert (\n",
    "    np.intersect1d(train_chrs, test_chrs, eval_chr).size == 0\n",
    "), \"Train and test chromosomes must be mutually exclusive\"\n",
    "\n",
    "# get train and test cells\n",
    "cell_sgkf = StratifiedGroupKFold(n_splits=5).split(\n",
    "    tune_data, tune_data[\"label_encoded\"], groups=tune_data[\"cell_id\"]\n",
    ")\n",
    "cell_train_index, cell_test_index = next(cell_sgkf)\n",
    "train_cells = tune_data.iloc[cell_train_index, :][\"cell_id\"].unique()\n",
    "test_cells = tune_data.iloc[cell_test_index, :][\"cell_id\"].unique()\n",
    "assert (\n",
    "    np.intersect1d(train_cells, test_cells).size == 0\n",
    "), \"Train and test cells must be mutually exclusive\"\n",
    "\n",
    "\n",
    "train_df = tune_data.loc[\n",
    "    (tune_data[\"Chromosome\"].isin(train_chrs))\n",
    "    & (tune_data[\"cell_id\"].isin(train_cells)),\n",
    "    :,\n",
    "]\n",
    "test_df = tune_data.loc[\n",
    "    (tune_data[\"Chromosome\"].isin(test_chrs)) & (tune_data[\"cell_id\"].isin(test_cells)),\n",
    "    :,\n",
    "]\n",
    "\n",
    "train_df = train_df.loc[train_df[\"nr1\"] >= 10, :]\n",
    "test_df = test_df.loc[test_df[\"nr1\"] >= 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit using holdout data\n",
    "clf = AutoML()\n",
    "\n",
    "clf.fit(\n",
    "    X_train=train_df[features],\n",
    "    y_train=train_df[\"label_encoded\"],\n",
    "    X_val=test_df[features],\n",
    "    y_val=test_df[\"label_encoded\"],\n",
    "    metric=\"f1\",\n",
    "    time_budget=600,\n",
    "    eval_method=\"holdout\",\n",
    "    log_file_name=\"flaml_holdout.log\",\n",
    "    **flaml_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for d in df[\"donor_id\"].unique():\n",
    "    val_data = eval_data.loc[(eval_data[\"donor_id\"] == d) & (eval_data[\"nr1\"] >= 10), :]\n",
    "    class_counts = val_data.label.value_counts().to_dict()\n",
    "    print(f\"donor {d} {eval_chr} windows: {val_data.shape[0]}\")\n",
    "    PrecisionRecallDisplay.from_estimator(\n",
    "        clf, val_data[features], val_data[\"label_encoded\"], name=f\"donor {d}\", ax=ax[0]\n",
    "    )\n",
    "    RocCurveDisplay.from_estimator(\n",
    "        clf, val_data[features], val_data[\"label_encoded\"], name=f\"donor {d}\", ax=ax[1]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
