import pandas as pd
from pathlib import Path
import os, shutil
from collections import defaultdict
from snakemake.utils import validate, min_version, Paramspace

##### set minimum snakemake version #####
min_version("7.22.0")


configfile: "config/config.yml"


validate(config, schema="schemas/config.schema.yaml")

# read sample sheets
samples = pd.read_csv(
    config["samples"],
    sep="\t",
    dtype={"sample_id": str, "tissue_id": str, "donor_id": str},
)
validate(samples, schema="schemas/samples.schema.yaml")
donors = pd.read_csv(config["donors"], sep="\t", dtype={"donor_id": str})
validate(donors, schema="schemas/donors.schema.yaml")

# merge sample sheets
samples = samples.merge(donors, on=["donor_id"]).set_index(
    ["donor_id", "sample_id"], drop=False
)

with open("resources/bad_cells.txt", "r") as f:
    bad_cells = [line.strip() for line in f.readlines()]

samples = samples[~samples["sample_id"].isin(bad_cells)]
cells = samples[
    ~samples["sample_id"].str.contains("gDNA")
    & ~samples["sample_id"].str.contains("Bulk")
]  # single-cells only, exclude bulk samples
bulk = samples[
    samples["sample_id"].str.contains("gDNA")
    | samples["sample_id"].str.contains("Bulk")
]  # bulk samples only, exclude single-cells

# create donor sheet
donors = donors.set_index("donor_id", drop=False)

# setup feature sets for model training
feature_sets = {
    "set1": [
        "n_reads",
        "n_proper_pairs",
        "n_duplicates",
        "num_supp_alignments_mean",
        "n_contigs",
        "alignment_score_normed_mean",
        "L1_alignment_score_normed_mean",
        "mate_alignment_score_normed_mean",
        "orientation_bias",
        "n_unique_5end",
        "n_unique_3end",
    ],
    "set2": [
        "frac_proper_pairs",
        "frac_duplicates",
        "frac_mean_supp_alignments",
        "frac_contigs",
        "alignment_score_normed_mean",
        "L1_alignment_score_normed_mean",
        "mate_alignment_score_normed_mean",
        "orientation_bias",
        "frac_unique_5end",
        "frac_unique_3end",
    ],
    "set3": [
        "n_reads",
        "n_duplicates",
        "n_unique_5end",
        "n_unique_3end",
        "orientation_bias",
    ],
}

extra = defaultdict(list)
for bg in ["bg5000", "bg10000", "bg20000"]:
    for s in feature_sets.keys():
        for f in feature_sets[s]:
            extra[s].append(bg + "_" + f)

feature_sets["set1"] = feature_sets["set1"] + extra["set1"]
feature_sets["set2"] = feature_sets["set2"] + extra["set2"]
feature_sets["set3"] = feature_sets["set3"] + extra["set3"]
feature_sets["set1_germdist"] = feature_sets["set1"] + ["germline_dist"]
feature_sets["set2_germdist"] = feature_sets["set2"] + ["germline_dist"]
feature_sets["set3_germdist"] = feature_sets["set3"] + ["germline_dist"]


wildcard_constraints:
    donor="|".join(donors["donor_id"]),
    sample="|".join(samples["sample_id"]),
    genome=config["genome"]["name"],
    outdir=config["outdir"],


include: "rules/ref.smk"
include: "rules/trim.smk"
include: "rules/align.smk"
include: "rules/regions.smk"


# include: "rules/peaks.smk"
# include: "rules/windows.smk"
# include: "rules/greedy_windows.smk"
# include: "rules/breakpoints.smk"
# include: "rules/model.smk"
# include: "rules/validate.smk"


rule map:
    input:
        expand(
            expand(
                rules.sort.output,
                zip,
                sample=samples["sample_id"],
                donor=samples["donor_id"],
                allow_missing=True,
            ),
            reads="filtered",
            genome=config["genome"]["name"],
            outdir=config["outdir"],
        ),


rule qc:
    input:
        expand(
            rules.reads_report.output,
            reads="filtered",
            genome=config["genome"]["name"],
            outdir=config["outdir"],
        ),


# rule model:
#     input:
#         expand(
#             rules.calls_report.output,
#             reads="filtered",
#             features=feature_sets.keys(),
#             genome=config["genome"]["name"],
#             outdir=config["outdir"],
#         ),
# rule igv:
#     input:
#         expand(
#             rules.igv_snapshots.output,
#             reads="filtered",
#             genome=config["genome"]["name"],
#             outdir=config["outdir"],
#             donor=donors["donor_id"].unique(),
#         ),
# rule validate:
#     input:
#         expand(
#             rules.wgs_igv.output,
#             reads="filtered",
#             genome=config["genome"]["name"],
#             outdir=config["outdir"],
#         ),
