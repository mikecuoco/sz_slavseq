{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sensitivity of bulk and single-cell SLAVseq signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import pyranges as pr\n",
    "from upsetplot import UpSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(snakemake.config[\"donors\"], sep=\"\\t\", dtype={\"donor_id\": str})  # type: ignore\n",
    "\n",
    "bdata, ldata, mdata = [], [], []\n",
    "l1hs = pr.read_bed(snakemake.input.l1hs_rmsk)  # type: ignore\n",
    "for b, m in zip(snakemake.input.bulk, snakemake.input.megane):  # type: ignore\n",
    "    assert (\n",
    "        Path(m).parent.name == Path(b).parent.name\n",
    "    ), \"filenames are not sorted properly\"\n",
    "    bdf = pd.read_csv(b, sep=\"\\t\").query(\"n_reads >= 5\")\n",
    "    bdf.columns = bdf.columns.str.replace(\"#\", \"\")\n",
    "    bdf[\"donor_id\"] = Path(b).parent.name\n",
    "    bdata.append(bdf)\n",
    "\n",
    "    # megane\n",
    "    meg = pr.read_bed(m).df\n",
    "    meg[\"AC\"] = meg[\"Strand\"].astype(int)\n",
    "    meg_l1hs = meg[\"Score\"].str.contains(\"L1HS\")\n",
    "    meg = pr.PyRanges(meg[meg_l1hs])\n",
    "    mdf = meg.join(\n",
    "        pr.PyRanges(bdf[[\"Chromosome\", \"Start\", \"End\", \"n_reads\", \"n_proper_pairs\"]]),\n",
    "        how=\"left\",\n",
    "    ).df\n",
    "    mdf[\"n_reads\"] = mdf[\"n_reads\"].apply(lambda x: 0 if x < 0 else x)\n",
    "    mdf[\"n_proper_pairs\"] = mdf[\"n_proper_pairs\"].apply(lambda x: 0 if x < 0 else x)\n",
    "    mdf[\"donor_id\"] = Path(m).parent.name\n",
    "    mdata.append(mdf)\n",
    "\n",
    "    # l1hs\n",
    "    ldf = l1hs.join(\n",
    "        pr.PyRanges(bdf[[\"Chromosome\", \"Start\", \"End\", \"n_reads\", \"n_proper_pairs\"]]),\n",
    "        how=\"left\",\n",
    "    ).df\n",
    "    ldf[\"n_reads\"] = ldf[\"n_reads\"].apply(lambda x: 0 if x < 0 else x)\n",
    "    ldf[\"n_proper_pairs\"] = ldf[\"n_proper_pairs\"].apply(lambda x: 0 if x < 0 else x)\n",
    "    ldf[\"donor_id\"] = Path(b).parent.name\n",
    "    ldata.append(ldf)\n",
    "\n",
    "ldata = pd.concat(ldata).merge(meta, on=\"donor_id\")\n",
    "ldata[\"locus\"] = tuple(zip(ldata[\"Chromosome\"], ldata[\"Start\"], ldata[\"End\"]))\n",
    "ldata = ldata.groupby([\"donor_id\", \"race\", \"locus\"])[\"n_reads\"].max().reset_index()\n",
    "ldata[\"locus\"] = ldata[\"locus\"].astype(str)\n",
    "mdata = pd.concat(mdata).merge(meta, on=\"donor_id\")\n",
    "mdata[\"locus\"] = tuple(zip(mdata[\"Chromosome\"], mdata[\"Start\"], mdata[\"End\"]))\n",
    "mdata = (\n",
    "    mdata.groupby([\"donor_id\", \"race\", \"locus\", \"AC\"])[\"n_reads\"].max().reset_index()\n",
    ")\n",
    "mdata[\"locus\"] = mdata[\"locus\"].astype(str)\n",
    "bdata = pd.concat(bdata).merge(meta, on=\"donor_id\")\n",
    "bdata[\"Width\"] = bdata[\"End\"] - bdata[\"Start\"]\n",
    "print(f\"Loaded {len(bdata)} peaks from {bdata['donor_id'].nunique()} donors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf plots\n",
    "g, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "cols = sns.color_palette(\"tab10\", n_colors=2)\n",
    "\n",
    "opts = {\n",
    "    \"hue_order\": [\"CAUC\", \"AA\"],\n",
    "    \"hue\": \"race\",\n",
    "    \"palette\": {\"CAUC\": cols[0], \"AA\": cols[1]},\n",
    "    \"alpha\": 0.5,\n",
    "    \"stat\": \"count\",\n",
    "    \"log_scale\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for d in bdata[\"donor_id\"].unique():\n",
    "    for ax, data in zip(axs, [ldata, mdata]):\n",
    "        df = data.query(\"donor_id == @d\")\n",
    "        df = df.loc[df.groupby(\"locus\")[\"n_reads\"].idxmax()]\n",
    "        # add one for log scale\n",
    "        df[\"n_reads\"] = df[\"n_reads\"] + 1\n",
    "        sns.ecdfplot(df, x=\"n_reads\", ax=ax, **opts)\n",
    "\n",
    "axs[0].set_title(\"# Reference L1HS\")\n",
    "axs[1].set_title(\"# Non-Reference L1HS (detected from WGS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots\n",
    "g, axs = plt.subplots(2, 1, figsize=(16, 16))\n",
    "\n",
    "for data, ax in zip([ldata, mdata], axs):\n",
    "\n",
    "    locus_order = data.groupby([\"locus\"])[\"n_reads\"].mean().sort_values().index\n",
    "\n",
    "    df = data.groupby([\"locus\", \"donor_id\"])[\"n_reads\"].max().reset_index()\n",
    "    df = df.set_index(\"locus\").loc[locus_order].reset_index()\n",
    "    df[\"locus\"] = df[\"locus\"].astype(str)\n",
    "    sns.boxplot(\n",
    "        data=df, x=\"locus\", y=\"n_reads\", showfliers=False, ax=ax, log_scale=True\n",
    "    )\n",
    "\n",
    "    # remove xtick labels\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "axs[0].set_title(\"Reference L1HS\")\n",
    "axs[1].set_title(\"Non-Reference L1HS (detected from WGS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRGL heatmap\n",
    "df = (\n",
    "    ldata.groupby([\"locus\", \"donor_id\"])[\"n_reads\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .pivot_table(index=\"locus\", columns=\"donor_id\", values=\"n_reads\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "log_norm = colors.LogNorm(vmin=df.min().min() + 1, vmax=df.max().max())\n",
    "sns.clustermap(df, cmap=\"viridis\", norm=log_norm, yticklabels=False, method=\"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNRGL heatmap\n",
    "assert len(mdata[[\"locus\", \"donor_id\"]]) == len(\n",
    "    mdata[[\"locus\", \"donor_id\"]].drop_duplicates()\n",
    "), \"duplicate locus-donor pairs found!\"\n",
    "locus_order = mdata.groupby([\"locus\"])[\"AC\"].sum().sort_values().index\n",
    "\n",
    "df = (\n",
    "    mdata.groupby([\"locus\", \"donor_id\"])[\"n_reads\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .pivot_table(index=\"locus\", columns=\"donor_id\", values=\"n_reads\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "log_norm = colors.LogNorm(vmin=df.min().min() + 1, vmax=df.max().max())\n",
    "sns.clustermap(\n",
    "    df.loc[locus_order],\n",
    "    cmap=\"viridis\",\n",
    "    norm=log_norm,\n",
    "    yticklabels=False,\n",
    "    row_cluster=False,\n",
    "    col_cluster=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, ldata, mdata = [], [], []\n",
    "\n",
    "# iterate over donors\n",
    "with tqdm(total=len(snakemake.input.cells)) as pbar:\n",
    "    for m in snakemake.input.megane:  # type: ignore\n",
    "        meg = pr.read_bed(m).df\n",
    "        meg_l1hs = meg[\"Score\"].str.contains(\"L1HS\")\n",
    "        meg = pr.PyRanges(meg[meg_l1hs])\n",
    "\n",
    "        # get cells\n",
    "        donor_id = Path(m).parent.name\n",
    "\n",
    "        # iterate over cells\n",
    "        donor_cells = [\n",
    "            c for c in snakemake.input.cells if Path(c).parent.name == donor_id\n",
    "        ]\n",
    "        print(f\"Found {len(donor_cells)} cells for donor {donor_id}\")\n",
    "        for c in donor_cells:\n",
    "            pbar.update()\n",
    "            df = pd.read_csv(c, sep=\"\\t\").query(\"n_reads >= 5\")\n",
    "            df.columns = df.columns.str.replace(\"#\", \"\")\n",
    "            df[\"donor_id\"] = Path(c).parent.name\n",
    "            df[\"cell_id\"] = Path(c).name.rstrip(\".labelled.bed.gz\")\n",
    "            data.append(df)\n",
    "\n",
    "            # megane\n",
    "            mdf = meg.join(\n",
    "                pr.PyRanges(\n",
    "                    df[[\"Chromosome\", \"Start\", \"End\", \"n_reads\", \"n_proper_pairs\"]]\n",
    "                ),\n",
    "                how=\"left\",\n",
    "            ).df\n",
    "            mdf[\"n_reads\"] = mdf[\"n_reads\"].apply(lambda x: 0 if x < 0 else x)\n",
    "            mdf[\"n_proper_pairs\"] = mdf[\"n_proper_pairs\"].apply(\n",
    "                lambda x: 0 if x < 0 else x\n",
    "            )\n",
    "            mdf[\"donor_id\"] = Path(m).parent.name\n",
    "            mdf[\"cell_id\"] = Path(c).name.rstrip(\".labelled.bed.gz\")\n",
    "            mdata.append(mdf)\n",
    "\n",
    "            # l1hs\n",
    "            ldf = l1hs.join(\n",
    "                pr.PyRanges(\n",
    "                    df[[\"Chromosome\", \"Start\", \"End\", \"n_reads\", \"n_proper_pairs\"]]\n",
    "                ),\n",
    "                how=\"left\",\n",
    "            ).df\n",
    "            ldf[\"n_reads\"] = ldf[\"n_reads\"].apply(lambda x: 0 if x < 0 else x)\n",
    "            ldf[\"n_proper_pairs\"] = ldf[\"n_proper_pairs\"].apply(\n",
    "                lambda x: 0 if x < 0 else x\n",
    "            )\n",
    "            ldf[\"donor_id\"] = Path(m).parent.name\n",
    "            ldf[\"cell_id\"] = Path(c).name.rstrip(\".labelled.bed.gz\")\n",
    "            ldata.append(ldf)\n",
    "\n",
    "ldata = pd.concat(ldata).merge(meta, on=\"donor_id\")\n",
    "ldata[\"locus\"] = tuple(zip(ldata[\"Chromosome\"], ldata[\"Start\"], ldata[\"End\"]))\n",
    "mdata = pd.concat(mdata).merge(meta, on=\"donor_id\")\n",
    "mdata[\"locus\"] = tuple(zip(mdata[\"Chromosome\"], mdata[\"Start\"], mdata[\"End\"]))\n",
    "data = pd.concat(data).merge(meta, on=\"donor_id\")\n",
    "data[\"Width\"] = data[\"End\"] - data[\"Start\"]\n",
    "print(\n",
    "    f\"Loaded {len(data)} peaks from {data['cell_id'].nunique()} cells from {data['donor_id'].nunique()} donors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = (\n",
    "    ldata.groupby([\"cell_id\", \"donor_id\", \"race\", \"diagnosis\"])\n",
    "    .apply(lambda x: sum(x[\"n_reads\"] > 0) / len(x))\n",
    "    .reset_index(name=\"sensitivity\")\n",
    ")\n",
    "\n",
    "mdf = (\n",
    "    mdata.groupby([\"cell_id\", \"donor_id\", \"race\", \"diagnosis\"])\n",
    "    .apply(lambda x: sum(x[\"n_reads\"] > 0) / len(x))\n",
    "    .reset_index(name=\"sensitivity\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this 2d?\n",
    "g, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharey=True)\n",
    "plt.subplots_adjust(wspace=0.1)  # Adjust to your needs\n",
    "\n",
    "sns.histplot(ldf, x=\"sensitivity\", bins=100, ax=ax1)\n",
    "ax1.set_title(\"Reference L1HS\")\n",
    "ax1.set_xlabel(\"% convered (sensitivity)\")\n",
    "ax1.set_ylabel(\"# cells\")\n",
    "\n",
    "sns.histplot(data=mdf, x=\"sensitivity\", bins=100, ax=ax2)\n",
    "ax2.set_title(\"Non-Reference L1HS (detected from WGS)\")\n",
    "ax2.set_xlabel(\"% convered (sensitivity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), sharey=True)\n",
    "plt.subplots_adjust(wspace=0.1)  # Adjust to your needs\n",
    "\n",
    "sns.boxplot(data=ldf, x=\"sensitivity\", y=\"donor_id\", hue=\"race\", ax=ax1)\n",
    "sns.boxplot(data=mdf, x=\"sensitivity\", y=\"donor_id\", hue=\"race\", ax=ax2)\n",
    "ax1.set_title(\"Reference L1HS\")\n",
    "ax1.set_xlabel(\"% convered (sensitivity)\")\n",
    "ax2.set_title(\"Non-Reference L1HS (detected from WGS)\")\n",
    "ax2.set_xlabel(\"% convered (sensitivity)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
