{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import gzip, re\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect paths to fastq files for each sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_reads(file):\n",
    "    \"\"\"get number of reads in an open file\"\"\"\n",
    "    return int((1 + sum(1 for _ in file)) / 4)\n",
    "\n",
    "\n",
    "def file_info(filename):\n",
    "    \"\"\"get info for a gzipped file\"\"\"\n",
    "    first = None\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        first = f.readline().rstrip()\n",
    "        # nreads = num_reads(f)\n",
    "\n",
    "    # return (filename, first, nreads)\n",
    "    return (filename, first)\n",
    "\n",
    "\n",
    "# define functions for file meta data generate from file name\n",
    "def fields(filename):\n",
    "    \"\"\"parse filename to get relevant fields\"\"\"\n",
    "    m = re.search(r\"/([^/]+?)(_001)?.fastq.gz$\", filename)\n",
    "    assert m is not None\n",
    "    basename = m.group(1)\n",
    "\n",
    "    r = dict()\n",
    "    r[\"filename\"] = filename\n",
    "    r[\"basename\"] = basename\n",
    "    r[\"pair_id\"] = re.sub(\"_(R[12])$\", \"\", basename)\n",
    "\n",
    "    if basename.startswith(\"plate\"):\n",
    "        m2 = re.search(r\"^(plate\\d+)_([A-H]\\d+)_(S\\d+)_(R[12])$\", basename)\n",
    "        assert m2 is not None\n",
    "        r[\"individual\"] = \"CommonBrain\"\n",
    "        r[\"sample_id1\"] = m2.group(1).lower() + m2.group(2).upper()\n",
    "        r[\"sample_id2\"] = m2.group(3).upper()\n",
    "        r[\"read\"] = m2.group(4).upper()\n",
    "        r[\"tissue\"] = \"brain\"\n",
    "        r[\"tissue_id\"] = \"CommonBrain\"\n",
    "        r[\"sample_type\"] = \"single_cell\"\n",
    "        r[\"dna_type\"] = \"mda\"\n",
    "    elif basename.upper().startswith(\"US\"):\n",
    "        m2 = re.search(\n",
    "            r\"^(US([DH])(\\d+))_?([A-H]\\d+)_(S\\d+)_(R[12])$\",\n",
    "            basename,\n",
    "            flags=re.IGNORECASE,\n",
    "        )\n",
    "        r[\"individual\"] = re.sub(\"^0+\", \"\", m2.group(3))\n",
    "        r[\"sample_id1\"] = m2.group(4).upper()\n",
    "        r[\"sample_id2\"] = m2.group(5).upper()\n",
    "        r[\"read\"] = m2.group(6).upper()\n",
    "        r[\"tissue\"] = \"HIPPO\" if m2.group(2).upper() == \"H\" else \"DLPFC\"\n",
    "        ind = r[\"individual\"] if int(r[\"individual\"]) >= 10 else \"0\" + r[\"individual\"]\n",
    "        r[\"tissue_id\"] = \"US\" + m2.group(2).upper() + ind\n",
    "        r[\"sample_type\"] = \"single_cell\"\n",
    "        r[\"dna_type\"] = \"mda\"\n",
    "    elif basename.startswith(\"gDNA\"):\n",
    "        m2 = re.search(r\"^gDNA_(US([DH])(\\d+))_(R[12])$\", basename, flags=re.IGNORECASE)\n",
    "        r[\"individual\"] = re.sub(\"^0+\", \"\", m2.group(3))\n",
    "        r[\"sample_id1\"] = \"bulk\"\n",
    "        r[\"sample_id2\"] = \"Sbulk\"\n",
    "        r[\"read\"] = m2.group(4).upper()\n",
    "        r[\"tissue\"] = \"HIPPO\" if m2.group(2).upper() == \"H\" else \"DLPFC\"\n",
    "        ind = r[\"individual\"] if int(r[\"individual\"]) >= 10 else \"0\" + r[\"individual\"]\n",
    "        r[\"tissue_id\"] = \"US\" + m2.group(2).upper() + ind\n",
    "        r[\"sample_type\"] = \"bulk\"\n",
    "        r[\"dna_type\"] = \"bulk\"\n",
    "    elif \"Bulk\" in basename:\n",
    "        r[\"individual\"] = \"CommonBrain\"\n",
    "        r[\"sample_id1\"] = \"bulk\"\n",
    "        r[\"sample_id2\"] = \"Sbulk\"\n",
    "        r[\"read\"] = \"R1\" if \"R1\" in basename else \"R2\"\n",
    "        r[\"tissue\"] = \"DLPFC\" if \"cor\" in basename else \"fibroblast\"\n",
    "        r[\"tissue_id\"] = \"CommonBrain\"\n",
    "        r[\"sample_type\"] = \"bulk\"\n",
    "        r[\"dna_type\"] = \"bulk\"\n",
    "    else:\n",
    "        r = None\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def find_logg_files():\n",
    "    \"\"\"\n",
    "    Find fastq files on filepaths\n",
    "    \"\"\"\n",
    "\n",
    "    # glob for files\n",
    "    files1 = Path(\"/iblm/logglun01/fqiu/AWS/for-ndar/SLAV-Seq/\").rglob(\"**/*fastq.gz\")\n",
    "    files2 = Path(\"/iblm/logglun02/BSMN/SLAV-Seq/\").rglob(\"**/*fastq.gz\")\n",
    "    files3 = Path(\"/iblm/netapp/data3/mcuoco/sz_slavseq/data/\").rglob(\"*Bulk*fastq.gz\")\n",
    "\n",
    "    # add generators together\n",
    "    files = list(files1) + list(files2) + list(files3)\n",
    "\n",
    "    # use threads because first_line is i/o bound\n",
    "    results = [file_info(str(f)) for f in tqdm(files)]\n",
    "    print(\"found {} files\".format(len(results)))\n",
    "\n",
    "    # convert to dataframe\n",
    "    print(\"extracting first read from each file\")\n",
    "    logg_files = pd.DataFrame.from_records(results, columns=[\"filename\", \"first_read\"])\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "\t\t{logg_files.shape[0]} files\n",
    "\t\t{len(set(logg_files['first_read']))} unique files based on first read\n",
    "\t\t\"\"\"\n",
    "    )\n",
    "\n",
    "    # remove duplicates\n",
    "    logg_files = logg_files.drop_duplicates(subset=[\"first_read\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    # logg_files.set_index(\"filename\", inplace=True, drop=False)\n",
    "\n",
    "    # extract sample metadata from filenames\n",
    "    logg_files = pd.DataFrame.from_records(\n",
    "        [dict(fields(x)) for x in logg_files[\"filename\"]]\n",
    "    ).merge(logg_files, on=\"filename\")\n",
    "\n",
    "    # unstack read1 and read2\n",
    "    logg_files = (\n",
    "        logg_files.pivot(\n",
    "            columns=\"read\",\n",
    "            values=\"filename\",\n",
    "            index=[\"pair_id\", \"individual\", \"dna_type\", \"tissue_id\"],\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={\"pair_id\": \"sample_id\", \"individual\": \"donor_id\"})\n",
    "        .dropna(subset=[\"R1\", \"R2\"])\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(logg_files)} unique samples with both R1 and R2 reads\")\n",
    "\n",
    "    return logg_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the samplesheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8694111754499da166338daff5025e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 14098 files\n",
      "extracting first read from each file\n",
      "\n",
      "\t\t14098 files\n",
      "\t\t8886 unique files based on first read\n",
      "\t\t\n",
      "Found 4438 unique samples with both R1 and R2 reads\n"
     ]
    }
   ],
   "source": [
    "meta = pd.read_csv(\"slavseq_metadata.tsv\", sep=\"\\t\").drop(\n",
    "    [\"MDA_PERFORMED\", \"BULK_PERFORMED\"], axis=1\n",
    ")\n",
    "meta.columns = meta.columns.str.lower()\n",
    "meta.set_index(\"tissue_id\", inplace=True)\n",
    "meta.loc[\"CommonBrain\", \"libd_id\"] = \"CommonBrain\"\n",
    "\n",
    "# cleanup\n",
    "# extract sample metadata from filenames\n",
    "logg = find_logg_files().join(meta, on=\"tissue_id\", how=\"left\")  # join with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create donor sheeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create donorsheet\n",
    "donors = logg[\n",
    "    [\"donor_id\", \"brain_id\", \"sex\", \"age\", \"libd_id\", \"race\", \"diagnosis\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "donors.loc[donors[\"donor_id\"] == \"CommonBrain\", \"libd_id\"] = \"CommonBrain\"\n",
    "# switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the L1 breakpoint files\n",
    "files = Path(\"../resources/chm13v2.0.XY/wgs_calls/30x/\").rglob(\n",
    "    \"*breakpoint_pairs_pooled_all.txt.gz\"\n",
    ")\n",
    "knrgl = {f.parts[-2]: str(f.resolve()) for f in files}\n",
    "donors[\"breakpoints\"] = donors[\"libd_id\"].map(knrgl)\n",
    "\n",
    "# find the 30x and 90x megane calls\n",
    "files = Path(\"../resources/chm13v2.0.XY/wgs_calls/30x/\").rglob(\n",
    "    \"*MEI_final_gaussian_genotyped.bed\"\n",
    ")\n",
    "knrgl = {f.parts[-2]: str(f.resolve()) for f in files}\n",
    "donors[\"megane_30x\"] = donors[\"libd_id\"].map(knrgl)\n",
    "\n",
    "files = Path(\"../resources/chm13v2.0.XY/wgs_calls/90x/\").rglob(\n",
    "    \"*MEI_final_gaussian_genotyped.bed\"\n",
    ")\n",
    "meta_90x = pd.read_csv(\"../config/U01_LIBD_wgs.tsv\", sep=\"\\t\")\n",
    "for f in files:\n",
    "    tissue_id = f.parts[-2]\n",
    "    if tissue_id in meta_90x[\"WGS_90x_DLPFC\"].values:\n",
    "        libd_id = meta_90x.query(\"WGS_90x_DLPFC == @tissue_id\")[\"ID\"].values[0]\n",
    "        donors.loc[donors[\"libd_id\"] == libd_id, \"megane_90x_DLPFC\"] = str(f.resolve())\n",
    "    elif tissue_id in meta_90x[\"WGS_90x_HIPPO\"].values:\n",
    "        libd_id = meta_90x.query(\"WGS_90x_HIPPO == @tissue_id\")[\"ID\"].values[0]\n",
    "        donors.loc[donors[\"libd_id\"] == libd_id, \"megane_90x_HIPPO\"] = str(f.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write\n",
    "\n",
    "# everyone\n",
    "logg.to_csv(\"all_samples.tsv\", sep=\"\\t\", index=False)\n",
    "donors.to_csv(\"all_donors.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# write donor sheets to thirds\n",
    "df1, df2, df3 = np.array_split(donors, 3)\n",
    "\n",
    "\n",
    "# Write each part to a separate CSV file\n",
    "df1.to_csv(\"onethird_donors1.tsv\", index=False, sep=\"\\t\")\n",
    "df2.to_csv(\"onethird_donors2.tsv\", index=False, sep=\"\\t\")\n",
    "df3.to_csv(\"onethird_donors3.tsv\", index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
