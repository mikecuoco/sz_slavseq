{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect paths to fastq files for each sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import gzip, re\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_reads(file):\n",
    "    \"\"\"get number of reads in an open file\"\"\"\n",
    "    return int((1 + sum(1 for _ in file)) / 4)\n",
    "\n",
    "\n",
    "def file_info(filename):\n",
    "    \"\"\"get info for a gzipped file\"\"\"\n",
    "    first = None\n",
    "    with gzip.open(filename, \"rt\") as f:\n",
    "        first = f.readline().rstrip()\n",
    "        # nreads = num_reads(f)\n",
    "\n",
    "    # return (filename, first, nreads)\n",
    "    return (filename, first)\n",
    "\n",
    "\n",
    "# define functions for file meta data generate from file name\n",
    "def fields(filename):\n",
    "    \"\"\"parse filename to get relevant fields\"\"\"\n",
    "    m = re.search(\"/([^/]+?)(_001)?.fastq.gz$\", filename)\n",
    "    assert m is not None\n",
    "    basename = m.group(1)\n",
    "\n",
    "    r = dict()\n",
    "    r[\"filename\"] = filename\n",
    "    r[\"basename\"] = basename\n",
    "    r[\"pair_id\"] = re.sub(\"_(R[12])$\", \"\", basename)\n",
    "\n",
    "    if basename.startswith(\"plate\"):\n",
    "        m2 = re.search(\"^(plate\\d+)_([A-H]\\d+)_(S\\d+)_(R[12])$\", basename)\n",
    "        assert m2 is not None\n",
    "        r[\"individual\"] = \"CommonBrain\"\n",
    "        r[\"sample_id1\"] = m2.group(1).lower() + m2.group(2).upper()\n",
    "        r[\"sample_id2\"] = m2.group(3).upper()\n",
    "        r[\"read\"] = m2.group(4).upper()\n",
    "        r[\"tissue\"] = \"brain\"\n",
    "        r[\"tissue_id\"] = \"CommonBrain\"\n",
    "        r[\"sample_type\"] = \"single_cell\"\n",
    "        r[\"dna_type\"] = \"mda\"\n",
    "    elif basename.upper().startswith(\"US\"):\n",
    "        m2 = re.search(\n",
    "            \"^(US([DH])(\\d+))_?([A-H]\\d+)_(S\\d+)_(R[12])$\",\n",
    "            basename,\n",
    "            flags=re.IGNORECASE,\n",
    "        )\n",
    "        r[\"individual\"] = re.sub(\"^0+\", \"\", m2.group(3))\n",
    "        r[\"sample_id1\"] = m2.group(4).upper()\n",
    "        r[\"sample_id2\"] = m2.group(5).upper()\n",
    "        r[\"read\"] = m2.group(6).upper()\n",
    "        r[\"tissue\"] = \"HIPPO\" if m2.group(2).upper() == \"H\" else \"DLPFC\"\n",
    "        ind = r[\"individual\"] if int(r[\"individual\"]) >= 10 else \"0\" + r[\"individual\"]\n",
    "        r[\"tissue_id\"] = \"US\" + m2.group(2).upper() + ind\n",
    "        r[\"sample_type\"] = \"single_cell\"\n",
    "        r[\"dna_type\"] = \"mda\"\n",
    "    elif basename.startswith(\"gDNA\"):\n",
    "        m2 = re.search(\"^gDNA_(US([DH])(\\d+))_(R[12])$\", basename, flags=re.IGNORECASE)\n",
    "        r[\"individual\"] = re.sub(\"^0+\", \"\", m2.group(3))\n",
    "        r[\"sample_id1\"] = \"bulk\"\n",
    "        r[\"sample_id2\"] = \"Sbulk\"\n",
    "        r[\"read\"] = m2.group(4).upper()\n",
    "        r[\"tissue\"] = \"HIPPO\" if m2.group(2).upper() == \"H\" else \"DLPFC\"\n",
    "        ind = r[\"individual\"] if int(r[\"individual\"]) >= 10 else \"0\" + r[\"individual\"]\n",
    "        r[\"tissue_id\"] = \"US\" + m2.group(2).upper() + ind\n",
    "        r[\"sample_type\"] = \"bulk\"\n",
    "        r[\"dna_type\"] = \"bulk\"\n",
    "    else:\n",
    "        r = None\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def print_unique(df):\n",
    "    print(\n",
    "        f\"\"\"\n",
    "\t\t{df.shape[0]} files\n",
    "\t\t{len(set(df['first_read']))} unique files based on first read\n",
    "\t\t\"\"\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob for files\n",
    "files = glob.glob(\n",
    "    \"/raidixshare_logg01/fqiu/AWS/for-ndar/SLAV-Seq/**/*fastq.gz\", recursive=True\n",
    ") + glob.glob(\"/raidixshare_log-g/BSMN/SLAV-Seq/**/*fastq.gz\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14094/14094 [03:55<00:00, 59.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 14094 files\n"
     ]
    }
   ],
   "source": [
    "# use threads because first_line is i/o bound\n",
    "results = Parallel(n_jobs=4)(delayed(file_info)(f) for f in tqdm(files))\n",
    "print(\"found {} files\".format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting first read from each file\n",
      "\n",
      "\t\t14094 files\n",
      "\t\t8882 unique files based on first read\n",
      "\t\t\n"
     ]
    }
   ],
   "source": [
    "# convert to dataframe\n",
    "print(\"extracting first read from each file\")\n",
    "logg_files = pd.DataFrame.from_records(results, columns=[\"filename\", \"first_read\"])\n",
    "\n",
    "print_unique(logg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing duplicate fastqs\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "logg_files.drop_duplicates(subset=[\"first_read\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the samplesheet and donorsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sample metadata from filenames\n",
    "df = pd.DataFrame.from_records([dict(fields(x)) for x in logg_files[\"filename\"]]).merge(\n",
    "    logg_files, on=\"filename\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(\"slavseq_metadata.tsv\", sep=\"\\t\").drop(\n",
    "    [\"MDA_PERFORMED\", \"BULK_PERFORMED\"], axis=1\n",
    ")\n",
    "meta.columns = meta.columns.str.lower()\n",
    "meta.set_index(\"tissue_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "\n",
    "df = (\n",
    "    df.filter(\n",
    "        items=[\n",
    "            \"filename\",\n",
    "            \"read\",\n",
    "            \"individual\",\n",
    "            \"pair_id\",\n",
    "            \"dna_type\",\n",
    "            \"tissue_id\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )  # keep only relevant columns\n",
    "    .pivot(\n",
    "        columns=\"read\",\n",
    "        values=\"filename\",\n",
    "        index=[\"pair_id\", \"individual\", \"dna_type\", \"tissue_id\"],\n",
    "    )  # pivot to get R1 and R2 in same row\n",
    "    .reset_index()\n",
    "    .join(meta, on=\"tissue_id\", how=\"left\")  # join with metadata\n",
    "    .rename(columns={\"pair_id\": \"sample_id\", \"individual\": \"donor_id\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write samplesheet\n",
    "(\n",
    "    df.filter(\n",
    "        items=[\"sample_id\", \"donor_id\", \"dna_type\", \"tissue_id\", \"R1\", \"R2\"], axis=1\n",
    "    )\n",
    "    .loc[df[\"dna_type\"] == \"mda\"]\n",
    "    .loc[df[\"donor_id\"] == \"CommonBrain\"]\n",
    "    .drop([\"dna_type\"], axis=1)\n",
    "    .to_csv(\"commonbrain_samples.tsv\", sep=\"\\t\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write donorsheet\n",
    "(\n",
    "    df.filter(\n",
    "        items=[\"donor_id\", \"brain_id\", \"sex\", \"age\", \"libd_id\", \"race\", \"diagnosis\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    .loc[df[\"donor_id\"] == \"CommonBrain\"]\n",
    "    .drop_duplicates()\n",
    "    .to_csv(\"commonbrain_donors.tsv\", sep=\"\\t\", index=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
